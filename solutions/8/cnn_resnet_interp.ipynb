{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal, cast\n",
    "\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.models.resnet import ResNet\n",
    "from tqdm import tqdm\n",
    "from plotly import express as px, graph_objects as go\n",
    "\n",
    "from src import load_latest\n",
    "from src.utils import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/matthewbaggins/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/matthewbaggins/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/matthewbaggins/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model: ResNet = t.hub.load(\"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dog.jpg', <http.client.HTTPMessage at 0x7fb8b8947c90>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download an example image from the pytorch website\n",
    "from urllib import request\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6339e-02, -1.5506e+00,  3.2032e-01, -2.0590e+00, -8.5789e-01,\n",
      "         1.7853e+00,  1.4694e+00,  2.1625e+00,  4.4890e+00,  8.2922e-01,\n",
      "        -5.7825e+00, -3.4975e+00, -4.0629e+00, -4.7529e+00, -3.8074e+00,\n",
      "        -4.7250e+00, -1.2607e+00,  2.9643e-01, -2.0471e+00, -5.3099e-01,\n",
      "        -3.5982e+00, -8.1666e-01, -2.7665e+00, -1.2774e+00, -3.4204e+00,\n",
      "        -1.9035e+00, -3.0009e+00, -1.3474e+00, -1.8383e+00,  1.3947e+00,\n",
      "        -2.0109e+00, -1.4142e+00, -2.3272e+00, -1.8205e+00, -1.1873e-01,\n",
      "        -3.4104e+00, -1.6544e+00, -3.4491e+00, -2.6464e+00, -2.7418e+00,\n",
      "        -2.2195e+00, -3.6512e+00, -4.1251e+00, -5.5947e+00, -1.7520e+00,\n",
      "        -1.6909e+00, -9.8139e-01, -2.1243e+00, -3.5150e+00, -1.3327e+00,\n",
      "        -1.1343e+00, -1.1564e+00, -2.2390e-02, -8.5852e-01, -1.2917e+00,\n",
      "        -2.8688e+00,  6.5904e-01, -1.7179e+00, -1.2448e+00, -2.3358e+00,\n",
      "        -5.8504e-02, -1.9220e+00, -2.5971e+00, -1.8031e+00, -1.5120e+00,\n",
      "        -1.0845e+00, -4.0847e-01, -1.3099e+00, -9.4110e-01, -4.0616e+00,\n",
      "        -1.9033e+00, -6.0939e-01, -2.3420e+00, -2.5525e+00, -2.7560e+00,\n",
      "        -2.1573e+00, -3.0428e+00, -3.8195e+00,  1.9947e+00,  8.0753e-01,\n",
      "        -2.8858e+00, -4.6382e-01, -1.9110e+00, -1.8595e+00, -1.9892e-01,\n",
      "        -7.4801e-01, -2.3912e+00, -1.1259e+00, -2.0500e+00,  2.2921e+00,\n",
      "        -2.6387e+00, -3.6724e+00, -4.7537e+00, -2.8049e+00, -1.9314e+00,\n",
      "        -4.6468e+00, -2.5197e+00, -2.4855e+00, -3.0077e+00, -4.8818e-01,\n",
      "        -9.3534e-01, -2.6058e+00,  2.1375e+00, -2.6330e+00,  8.3530e+00,\n",
      "         8.9691e-01,  3.7542e+00, -3.4338e+00, -1.8968e+00, -4.2944e+00,\n",
      "         2.8972e-01, -2.3296e+00,  2.3829e+00,  4.1724e-02,  2.2312e-01,\n",
      "         7.5348e-01, -3.3083e+00, -1.4775e+00, -1.1789e+00, -1.9242e+00,\n",
      "        -1.8415e+00, -1.2771e+00, -1.0861e+00, -9.9629e-01,  1.5323e-01,\n",
      "        -1.4349e+00, -1.8049e+00,  1.0726e+00, -1.8630e+00,  2.6082e-01,\n",
      "        -1.5036e+00, -4.6526e+00, -1.9526e+00, -6.8754e+00, -1.6431e+00,\n",
      "        -3.2270e+00, -3.2733e+00, -3.8368e+00, -2.1765e+00, -3.5052e+00,\n",
      "        -4.7009e+00, -3.6283e+00, -4.8846e+00, -1.9900e+00,  2.6414e-01,\n",
      "        -6.7303e-01,  9.3454e-01, -2.2415e+00, -2.1350e+00, -4.8016e-01,\n",
      "        -2.6277e-01,  5.0762e+00,  4.4921e+00,  5.3008e+00,  5.1164e+00,\n",
      "         1.0429e+00,  5.4806e-02,  5.7181e+00,  2.8067e+00, -6.5379e-01,\n",
      "         7.1684e-01, -1.0806e+00, -8.4585e-01, -1.4041e+00, -4.8315e-01,\n",
      "        -4.0574e+00,  2.2625e-02, -1.7129e+00, -2.5303e-01,  5.0031e+00,\n",
      "         6.5755e+00,  3.8812e-01,  1.4243e-01,  3.1377e+00,  6.6828e+00,\n",
      "         2.2432e+00,  4.6771e-01,  3.2977e+00, -8.5083e-01,  1.1055e+00,\n",
      "         1.6826e+00,  3.0940e-02,  2.7675e+00,  1.0106e+00,  3.2922e+00,\n",
      "         6.0674e+00,  7.0710e+00,  5.1806e-01,  3.6325e+00,  2.7661e+00,\n",
      "         3.2225e+00, -9.3630e-01,  6.9493e+00,  4.2851e+00,  2.6162e+00,\n",
      "         1.6081e+00,  4.3853e-01,  1.0902e+00, -3.5152e-01,  6.1580e+00,\n",
      "         2.6465e+00,  1.6083e+00,  2.6777e+00,  1.0351e+01,  2.5237e+00,\n",
      "         8.3555e-01, -1.3495e+00,  5.1933e+00,  2.9645e+00, -5.6630e-01,\n",
      "        -1.4314e+00,  3.0180e-02,  2.4531e+00,  3.0547e-01, -7.7727e-01,\n",
      "         1.6614e+00,  1.9859e+00,  2.0599e+00,  2.7053e-01, -1.8844e-01,\n",
      "         2.7453e-01, -1.7493e+00,  8.8917e+00,  8.1119e+00,  7.3258e+00,\n",
      "         2.8675e+00,  3.7856e+00,  4.2008e+00,  5.8179e+00,  6.1480e+00,\n",
      "         6.4180e+00,  7.6096e+00,  6.9854e+00,  2.9950e+00, -5.2812e-01,\n",
      "         5.5978e+00,  1.4420e+00, -3.9359e-01,  2.0424e+00,  1.8702e+00,\n",
      "         2.0423e+00,  2.0490e-01,  6.4352e-01, -4.0694e-01,  2.5656e+00,\n",
      "         1.2415e+00,  1.4625e+00,  4.1671e+00,  1.0818e+01,  8.7363e+00,\n",
      "         9.9068e+00,  2.8379e+00,  2.0291e-01,  1.7992e+00,  2.3171e+00,\n",
      "         2.5287e+00,  4.5000e+00,  1.1025e+01,  1.6272e+01,  1.1213e+01,\n",
      "         7.8083e+00,  1.0739e+01, -9.8694e-02,  5.7145e+00,  4.7241e+00,\n",
      "         3.3937e+00,  3.0940e+00,  3.8410e+00, -8.0178e-01,  8.1751e+00,\n",
      "         1.3279e+01,  3.4680e+00,  4.4464e+00,  5.5256e+00,  4.1514e+00,\n",
      "        -1.9222e-01,  1.3210e+00,  5.0952e+00,  4.9230e+00,  1.3311e+01,\n",
      "         4.6685e+00,  4.1272e+00,  3.8468e+00,  6.4151e+00,  6.1105e+00,\n",
      "         3.3095e+00,  2.3559e+00,  6.8437e+00,  9.5330e-01,  2.8574e+00,\n",
      "        -1.1252e+00,  1.5081e+00,  1.7619e+00,  1.2515e+00,  2.8569e+00,\n",
      "         1.7482e+00,  6.7790e+00,  2.5266e-01, -1.3349e+00,  5.0950e-01,\n",
      "        -2.8949e+00, -8.2471e-01, -2.9116e+00, -3.0459e+00, -1.1134e+00,\n",
      "        -2.8015e+00, -1.4483e+00, -6.7259e-01, -4.2243e+00, -1.7617e+00,\n",
      "         4.3732e-01, -1.4013e+00, -1.5369e+00, -1.5598e+00,  1.6333e+00,\n",
      "        -2.3879e+00, -2.9763e+00, -4.3758e-01, -5.4941e-01, -6.1539e+00,\n",
      "        -4.0848e+00, -2.3562e+00, -3.9530e+00, -2.8771e+00, -4.5018e-01,\n",
      "        -1.7366e+00, -1.6378e+00,  2.1369e+00, -8.5072e-01, -1.5106e-02,\n",
      "         2.7718e+00,  5.3789e+00,  5.9544e+00,  5.4217e+00,  2.8849e+00,\n",
      "         3.7553e-01,  1.6293e+00,  4.4444e-01,  3.2406e+00,  9.7723e-01,\n",
      "         2.5465e-02,  3.1049e+00, -2.7004e-01, -2.7837e+00, -3.1689e+00,\n",
      "         8.1501e-01, -6.5138e-01, -2.9629e+00,  2.6049e+00, -9.6694e-01,\n",
      "        -9.5524e-01, -4.7054e+00, -3.5798e+00,  2.2927e-01, -2.1690e+00,\n",
      "         6.5018e+00,  5.7230e+00,  2.9804e+00,  6.4624e+00,  5.4993e+00,\n",
      "        -5.3051e-01,  4.7054e+00,  2.8012e+00, -2.6637e-03, -1.3485e+00,\n",
      "        -4.5583e-01, -5.6264e-01, -1.6322e+00,  2.1284e+00, -1.4563e+00,\n",
      "        -2.3063e-03,  9.0554e-01,  1.6597e+00,  2.0631e+00,  1.0822e+00,\n",
      "        -8.2019e-01, -3.6827e+00,  2.5450e+00,  1.4172e+00,  5.3829e-01,\n",
      "         1.0830e+00, -9.5248e-02,  4.6126e-01,  1.0738e+00,  7.6054e-02,\n",
      "        -2.6618e+00, -4.3085e+00,  3.6653e+00,  5.1768e+00, -6.1668e-01,\n",
      "        -1.5418e+00,  7.3493e-01, -3.7424e+00, -2.9974e+00, -5.4759e-01,\n",
      "        -1.9200e+00, -3.7740e+00, -2.9875e+00, -9.2509e-01, -4.2549e-01,\n",
      "        -8.8839e-01, -5.8605e-01, -6.5366e-01, -4.9164e+00, -2.8380e+00,\n",
      "         3.4216e-01, -2.2504e+00, -2.7396e-01, -2.4040e+00, -5.2708e-01,\n",
      "        -2.6042e+00,  3.2549e-01,  4.0671e+00, -2.1198e+00, -5.1570e-01,\n",
      "        -1.7743e+00, -2.0616e+00, -6.6488e-01, -1.4366e+00,  9.8567e-01,\n",
      "         5.4577e-03, -9.6882e-01, -6.0217e-01, -1.4005e+00, -2.0714e+00,\n",
      "         1.1076e+00, -1.9484e+00,  2.2077e+00,  2.9293e+00,  1.8656e+00,\n",
      "        -2.8894e+00, -4.3055e-01, -1.3991e+00, -3.7435e-01,  7.7693e-01,\n",
      "         3.9195e+00, -7.5799e-01, -1.8525e-01, -1.6258e+00,  2.2091e+00,\n",
      "         1.5951e+00,  6.9995e-02,  3.9133e-01,  4.6394e-01,  5.8918e-01,\n",
      "        -9.3522e-02, -5.1470e-01, -1.3623e+00, -1.5003e-01, -5.6841e-01,\n",
      "         2.4670e+00, -1.7122e+00, -1.6869e+00, -3.2666e-01, -8.4287e-01,\n",
      "         2.1149e+00, -8.3884e-02,  1.7850e+00, -6.3538e-01, -7.4758e-01,\n",
      "         4.2880e-01, -5.1257e-01,  3.4115e+00,  5.7136e+00, -1.2800e+00,\n",
      "        -1.6489e+00, -2.5274e+00, -3.1728e+00, -1.0106e+00,  9.4763e-01,\n",
      "         9.9265e-01, -1.1011e-01,  2.5838e+00,  1.4921e+00, -1.2985e+00,\n",
      "        -2.8229e-01, -8.6372e-01, -1.4782e-01,  1.7310e+00,  2.4503e+00,\n",
      "        -7.3577e-01, -2.5552e+00, -1.9138e+00,  1.6951e-01,  3.2679e-01,\n",
      "        -1.3447e+00, -1.5309e+00, -7.6593e-01, -7.8742e-01,  1.0069e+00,\n",
      "        -1.2147e+00,  1.4807e+00, -2.7379e-01, -1.3609e+00, -1.7789e-01,\n",
      "        -1.1743e+00,  2.3968e+00, -6.7979e-01, -2.6906e+00,  7.7297e-02,\n",
      "        -3.4516e+00,  1.2132e+00,  1.2684e+00, -1.1004e+00,  2.8001e-01,\n",
      "        -9.9931e-01, -2.0043e+00,  7.5322e-01,  2.4302e+00, -1.7247e+00,\n",
      "        -8.0332e-02,  8.8520e-01, -4.0822e-01, -2.4651e+00, -1.2022e+00,\n",
      "         5.4732e-01, -1.6022e+00, -2.4281e+00,  5.5507e-01, -3.8028e-01,\n",
      "        -2.0518e+00,  3.3781e+00,  3.1917e+00,  6.6947e-01, -6.8641e-01,\n",
      "        -9.4744e-01, -3.6536e-01, -7.6243e-01,  2.1818e+00,  8.0121e-01,\n",
      "        -1.0060e+00, -1.2830e+00, -2.3660e+00, -3.1692e+00,  2.1264e+00,\n",
      "         1.2476e-01, -1.6896e+00,  2.6978e+00, -3.0827e+00,  4.0319e+00,\n",
      "        -2.8332e+00,  3.3939e-01,  9.9010e-03,  5.2513e-01,  1.9750e+00,\n",
      "        -7.2911e-02,  2.8951e-01, -2.9114e+00, -2.4012e+00,  7.0123e-01,\n",
      "        -4.1851e+00,  4.7373e-01,  1.1475e+00, -6.1701e-01, -7.0963e-01,\n",
      "        -3.9961e-01, -3.3784e-01, -6.5776e-01, -2.2181e+00,  1.5349e+00,\n",
      "         1.2745e+00,  2.7791e-01, -2.2747e+00, -5.5950e-01, -3.7197e+00,\n",
      "        -2.9086e+00, -6.8770e-01,  9.6377e-02,  1.8657e+00, -8.5161e-01,\n",
      "        -8.4448e-01, -3.8533e+00,  3.4212e-01,  2.5622e-01,  2.6891e+00,\n",
      "         2.6102e-01, -2.0005e+00,  5.3860e-01,  1.0959e+00, -1.3286e+00,\n",
      "         1.4828e+00, -5.9474e-01, -2.0439e+00, -1.2363e+00,  1.0516e+00,\n",
      "         6.4886e-01, -6.2551e-02,  1.2362e+00,  6.8381e-01,  3.8256e+00,\n",
      "        -1.7969e+00, -6.7208e-01, -2.5611e+00, -1.6508e+00,  1.1806e+00,\n",
      "        -1.2919e+00,  2.2588e+00, -4.0449e-01, -1.1888e+00, -2.3773e+00,\n",
      "         7.9885e-01, -9.1904e-01, -1.6122e+00, -1.9350e+00, -3.9245e+00,\n",
      "        -2.1022e+00,  2.1339e+00, -2.5643e-01,  4.8948e-01, -6.0534e-01,\n",
      "         1.1570e+00,  1.8458e+00, -4.0937e+00,  2.7040e-01, -2.1655e-01,\n",
      "         1.3752e+00, -1.4046e+00, -1.0381e+00,  4.6964e-02, -1.2820e-01,\n",
      "         3.2887e-01,  1.8616e+00,  3.2686e+00, -3.5458e-01, -2.1411e+00,\n",
      "        -4.3288e-01,  7.7498e-01,  3.7516e-02, -2.0642e+00,  3.1101e-01,\n",
      "         1.6424e+00,  2.6678e+00,  2.7099e-01,  8.3660e-01, -2.9809e+00,\n",
      "        -2.4510e-01,  2.4835e-01,  1.3228e-01, -4.3413e-01, -4.3018e-01,\n",
      "         6.5715e-01,  2.1314e+00, -3.2655e+00, -5.1019e-01,  4.9308e-01,\n",
      "        -1.8939e+00,  1.8296e+00, -1.6717e+00,  6.3101e-01, -1.6439e+00,\n",
      "        -1.6592e+00, -1.1217e-03, -6.8726e-01,  2.0164e+00, -4.9025e-01,\n",
      "         7.9324e-01, -6.6057e-01, -1.4483e+00,  5.7437e-01,  2.2942e-01,\n",
      "        -2.9955e-01, -2.0788e+00, -9.1661e-01, -1.9485e+00,  8.3638e-01,\n",
      "         1.2686e+00, -9.1497e-01,  5.4699e-01, -3.6894e+00, -9.1424e-01,\n",
      "         1.9854e+00, -2.5371e-01,  2.8486e-01,  2.0943e+00,  1.2095e+00,\n",
      "         2.1252e-01,  3.4638e+00,  9.5287e-02, -8.0262e-01,  2.7175e-01,\n",
      "         1.2197e+00,  7.1921e-01, -2.8348e+00, -2.7134e+00,  4.7992e-01,\n",
      "        -1.8521e+00,  2.9714e-01, -4.1030e+00,  1.8298e-01, -7.9910e-01,\n",
      "        -1.7319e+00, -9.6755e-01, -5.8048e-01, -8.2791e-01, -9.8484e-02,\n",
      "        -5.0442e-01,  1.5539e+00,  1.4909e+00, -1.2977e+00, -1.0656e-01,\n",
      "         2.9900e+00, -1.3629e+00, -3.7234e+00,  9.1969e-01, -2.5767e+00,\n",
      "        -3.0631e+00,  1.3368e+00, -1.4010e+00, -3.5366e-01, -1.1713e+00,\n",
      "        -7.4930e-01, -2.6065e-01, -6.0239e-01, -1.5589e+00,  1.3902e+00,\n",
      "        -2.3780e+00,  1.9803e+00,  3.3025e-01, -1.0208e-01,  2.4418e+00,\n",
      "         1.2643e+00, -6.6208e-01,  2.2868e+00,  1.8490e+00, -2.9064e+00,\n",
      "        -5.7584e-01, -5.1065e+00, -2.6347e+00,  2.1374e+00, -1.3477e+00,\n",
      "        -2.2598e+00,  4.3973e+00, -2.2577e+00, -1.0246e+00, -3.2032e+00,\n",
      "         6.7235e-01, -1.9926e+00, -2.0801e-01,  4.0561e-01, -2.9341e+00,\n",
      "         1.5742e+00, -9.1935e-01,  1.1538e+00, -9.1431e-01, -1.2592e+00,\n",
      "        -1.9510e+00,  1.0337e+00,  8.3944e-01,  1.6421e+00,  1.3359e+00,\n",
      "         7.9254e-01,  7.8821e-01,  1.2222e+00,  1.3787e+00, -7.8177e-01,\n",
      "        -1.3663e+00,  5.3843e+00,  5.1326e-01,  4.9080e-01,  5.2627e-01,\n",
      "         7.3289e-01,  2.2716e+00, -1.3195e+00,  9.3247e-01, -1.6310e+00,\n",
      "         2.4205e-01, -2.4955e-01, -4.8593e-01,  2.1129e+00,  1.4489e+00,\n",
      "         3.7240e-01,  1.3994e+00,  1.5160e+00, -3.6225e-01,  2.9194e-01,\n",
      "        -9.4114e-01, -2.2435e+00, -4.9315e-01, -4.5941e-01, -2.1159e+00,\n",
      "        -2.6537e+00, -4.2153e-01,  1.4910e+00,  6.6824e-01,  1.0789e+00,\n",
      "        -2.9850e-01,  1.3671e+00,  7.1147e-01,  2.7865e-01,  3.5965e-02,\n",
      "        -1.5735e+00,  6.9061e-01,  1.3072e+00, -6.9291e-01,  8.5453e-02,\n",
      "        -4.8785e-01, -1.4151e+00,  2.3647e+00, -8.0707e-01,  7.6362e-01,\n",
      "        -4.9398e+00, -6.1584e-01, -4.6459e-01, -2.4253e+00,  1.1542e+00,\n",
      "         6.3761e+00,  9.0699e-01, -4.1544e-01, -1.3357e+00,  2.3733e-01,\n",
      "         1.4615e+00,  2.6385e+00, -1.3075e+00,  8.5217e-01, -3.5936e-01,\n",
      "        -9.9020e-01, -1.2833e+00,  5.6024e-01,  2.0276e-02, -8.9821e-01,\n",
      "        -6.9480e-01, -1.9415e+00, -6.7352e-01,  1.2533e+00,  1.3384e+00,\n",
      "         8.7576e-01, -4.7705e-01,  1.7547e+00,  5.0831e-01, -1.3910e+00,\n",
      "        -8.7806e-01,  1.1592e+00, -1.0183e+00, -1.3280e+00, -3.7179e-01,\n",
      "        -1.0465e+00,  5.7412e-01,  1.5570e+00,  1.8527e+00, -4.1109e-01,\n",
      "        -2.0666e-01,  1.0546e+00,  1.8594e+00,  4.5593e-01,  9.1500e-01,\n",
      "         1.1520e+00, -1.4328e+00, -5.3236e-01, -1.4753e+00, -1.0056e+00,\n",
      "         1.2255e+00,  1.5212e+00,  5.0189e+00, -1.5192e+00, -5.4166e-01,\n",
      "        -6.5469e-01, -1.8721e+00, -3.6343e+00, -1.0438e+00,  6.9751e-02,\n",
      "        -2.3699e+00,  2.4601e+00, -1.3046e-01, -1.7984e+00, -7.5459e-01,\n",
      "        -1.0365e+00, -1.3382e+00, -2.9626e-01, -8.2724e-01,  2.7181e-01,\n",
      "         1.5876e+00, -7.2424e-01, -2.9750e-01, -9.3013e-01, -2.4279e+00,\n",
      "        -9.2203e-01,  4.7529e+00, -1.6791e+00,  8.6737e-01,  7.8051e-01,\n",
      "         1.6629e+00, -1.0693e+00,  1.3566e+00, -8.2514e-01, -1.9551e+00,\n",
      "         4.4882e-01, -2.2731e+00, -1.9737e+00, -1.3142e+00,  7.3591e-02,\n",
      "        -6.2455e-01, -1.2942e+00, -7.9368e-01,  1.4236e-01, -5.3823e-01,\n",
      "        -3.7517e+00,  2.5791e+00,  2.9511e+00,  1.1155e+00, -7.3130e-02,\n",
      "        -1.0612e+00,  1.7574e-01,  1.4919e+00, -1.3878e+00,  1.2886e+00,\n",
      "        -1.8538e+00, -2.2473e+00,  2.7369e-01, -2.0514e+00, -5.4442e-01,\n",
      "         8.9385e-01,  7.9957e-01,  1.3955e+00, -1.7343e-01, -5.9457e-01,\n",
      "         8.0541e-01, -1.3384e-01, -3.2299e+00, -2.1461e-01, -1.5618e+00,\n",
      "        -2.2863e+00, -3.0846e-01, -4.4394e+00, -6.2356e-01, -2.5376e+00,\n",
      "        -2.4713e+00, -3.3116e+00, -2.8573e+00, -2.9199e+00,  3.9096e+00,\n",
      "        -2.2021e+00, -2.0263e+00, -5.1848e-01, -4.8115e+00, -2.6821e+00,\n",
      "         7.0437e-01, -3.2427e-01,  8.4757e-01,  1.2199e+00,  1.4956e+00,\n",
      "        -1.9149e+00, -3.8718e+00, -4.3360e-01,  2.0203e+00, -2.2132e+00,\n",
      "        -3.7776e+00, -3.2416e+00, -1.0162e+00,  1.8905e+00,  4.7865e-01,\n",
      "        -2.6048e+00, -2.1483e+00, -2.8170e+00, -1.2956e+00, -1.0286e+00,\n",
      "        -2.7622e+00, -1.6166e+00, -4.1225e-01, -1.9370e-01, -2.4866e+00,\n",
      "        -1.3858e+00,  7.6150e-01, -2.6262e+00, -2.1393e+00, -5.1699e+00,\n",
      "        -2.3784e+00, -4.5160e-01, -4.3731e+00, -2.3987e-01, -8.7328e-01,\n",
      "         4.7089e-01,  1.5141e+00, -1.1394e+00, -3.5783e+00, -1.3124e-01,\n",
      "         2.2075e+00, -1.6685e+00,  7.5993e-01, -1.0782e+00, -7.7672e-01,\n",
      "        -6.3270e-01,  1.1560e-01,  1.1363e+00, -1.4195e+00, -8.3308e-01,\n",
      "        -1.6161e+00, -2.2293e+00,  6.9339e-01, -3.4878e+00, -1.4843e+00,\n",
      "        -1.3018e+00, -1.8238e-01, -5.2429e-01, -5.7332e+00, -1.8340e+00,\n",
      "        -6.5485e-01, -1.8085e+00, -2.9124e+00,  5.6059e-01,  2.5123e+00],\n",
      "       device='cuda:0')\n",
      "tensor([7.7058e-08, 1.6080e-08, 1.0443e-07, 9.6720e-09, 3.2148e-08, 4.5194e-07,\n",
      "        3.2952e-07, 6.5901e-07, 6.7497e-06, 1.7372e-07, 2.3357e-10, 2.2950e-09,\n",
      "        1.3039e-09, 6.5400e-10, 1.6834e-09, 6.7251e-10, 2.1489e-08, 1.0197e-07,\n",
      "        9.7878e-09, 4.4577e-08, 2.0752e-09, 3.3501e-08, 4.7670e-09, 2.1133e-08,\n",
      "        2.4790e-09, 1.1299e-08, 3.7709e-09, 1.9705e-08, 1.2061e-08, 3.0580e-07,\n",
      "        1.0149e-08, 1.8431e-08, 7.3963e-09, 1.2277e-08, 6.7322e-08, 2.5037e-09,\n",
      "        1.4495e-08, 2.4087e-09, 5.3751e-09, 4.8864e-09, 8.2380e-09, 1.9681e-09,\n",
      "        1.2253e-09, 2.8181e-10, 1.3147e-08, 1.3976e-08, 2.8412e-08, 9.0607e-09,\n",
      "        2.2552e-09, 1.9996e-08, 2.4383e-08, 2.3850e-08, 7.4131e-08, 3.2127e-08,\n",
      "        2.0832e-08, 4.3036e-09, 1.4653e-07, 1.3603e-08, 2.1833e-08, 7.3331e-09,\n",
      "        7.1501e-08, 1.1092e-08, 5.6467e-09, 1.2493e-08, 1.6713e-08, 2.5628e-08,\n",
      "        5.0388e-08, 2.0457e-08, 2.9581e-08, 1.3056e-09, 1.1302e-08, 4.1216e-08,\n",
      "        7.2881e-09, 5.9048e-09, 4.8173e-09, 8.7661e-09, 3.6160e-09, 1.6631e-09,\n",
      "        5.5721e-07, 1.6999e-07, 4.2308e-09, 4.7675e-08, 1.1215e-08, 1.1807e-08,\n",
      "        6.2135e-08, 3.5881e-08, 6.9380e-09, 2.4589e-08, 9.7595e-09, 7.5017e-07,\n",
      "        5.4168e-09, 1.9268e-09, 6.5348e-10, 4.5873e-09, 1.0988e-08, 7.2719e-10,\n",
      "        6.1013e-09, 6.3135e-09, 3.7453e-09, 4.6528e-08, 2.9751e-08, 5.5979e-09,\n",
      "        6.4273e-07, 5.4480e-09, 3.2165e-04, 1.8589e-07, 3.2371e-06, 2.4458e-09,\n",
      "        1.1375e-08, 1.0344e-09, 1.0129e-07, 7.3791e-09, 8.2147e-07, 7.9039e-08,\n",
      "        9.4760e-08, 1.6105e-07, 2.7730e-09, 1.7300e-08, 2.3319e-08, 1.1067e-08,\n",
      "        1.2022e-08, 2.1140e-08, 2.5588e-08, 2.7993e-08, 8.8363e-08, 1.8053e-08,\n",
      "        1.2469e-08, 2.2159e-07, 1.1766e-08, 9.8400e-08, 1.6855e-08, 7.2297e-10,\n",
      "        1.0758e-08, 7.8302e-11, 1.4660e-08, 3.0079e-09, 2.8718e-09, 1.6347e-09,\n",
      "        8.6000e-09, 2.2775e-09, 6.8889e-10, 2.0136e-09, 5.7331e-10, 1.0363e-08,\n",
      "        9.8728e-08, 3.8675e-08, 1.9301e-07, 8.0586e-09, 8.9640e-09, 4.6902e-08,\n",
      "        5.8291e-08, 1.2142e-05, 6.7702e-06, 1.5200e-05, 1.2640e-05, 2.1509e-07,\n",
      "        8.0080e-08, 2.3070e-05, 1.2550e-06, 3.9426e-08, 1.5525e-07, 2.5730e-08,\n",
      "        3.2537e-08, 1.8618e-08, 4.6762e-08, 1.3110e-09, 7.7544e-08, 1.3671e-08,\n",
      "        5.8862e-08, 1.1286e-05, 5.4376e-05, 1.1176e-07, 8.7413e-08, 1.7475e-06,\n",
      "        6.0540e-05, 7.1438e-07, 1.2102e-07, 2.0508e-06, 3.2375e-08, 2.2900e-07,\n",
      "        4.0780e-07, 7.8192e-08, 1.2068e-06, 2.0828e-07, 2.0394e-06, 3.2717e-05,\n",
      "        8.9256e-05, 1.2727e-07, 2.8662e-06, 1.2052e-06, 1.9020e-06, 2.9723e-08,\n",
      "        7.9028e-05, 5.5045e-06, 1.0374e-06, 3.7852e-07, 1.1754e-07, 2.2552e-07,\n",
      "        5.3341e-08, 3.5819e-05, 1.0693e-06, 3.7860e-07, 1.1032e-06, 2.3724e-03,\n",
      "        9.4574e-07, 1.7482e-07, 1.9663e-08, 1.3651e-05, 1.4696e-06, 4.3031e-08,\n",
      "        1.8117e-08, 7.8132e-08, 8.8122e-07, 1.0289e-07, 3.4846e-08, 3.9926e-07,\n",
      "        5.5230e-07, 5.9471e-07, 9.9361e-08, 6.2789e-08, 9.9758e-08, 1.3183e-08,\n",
      "        5.5125e-04, 2.5273e-04, 1.1515e-04, 1.3338e-06, 3.3402e-06, 5.0596e-06,\n",
      "        2.5492e-05, 3.5462e-05, 4.6453e-05, 1.5294e-04, 8.1930e-05, 1.5151e-06,\n",
      "        4.4706e-08, 2.0455e-05, 3.2059e-07, 5.1143e-08, 5.8441e-07, 4.9199e-07,\n",
      "        5.8436e-07, 9.3049e-08, 1.4428e-07, 5.0465e-08, 9.8613e-07, 2.6236e-07,\n",
      "        3.2724e-07, 4.8919e-06, 3.7852e-03, 4.7190e-04, 1.5213e-03, 1.2948e-06,\n",
      "        9.2863e-08, 4.5826e-07, 7.6921e-07, 9.5042e-07, 6.8242e-06, 4.6551e-03,\n",
      "        8.8459e-01, 5.6182e-03, 1.8657e-04, 3.4964e-03, 6.8685e-08, 2.2987e-05,\n",
      "        8.5385e-06, 2.2572e-06, 1.6728e-06, 3.5304e-06, 3.4003e-08, 2.6924e-04,\n",
      "        4.4350e-02, 2.4314e-06, 6.4680e-06, 1.9031e-05, 4.8157e-06, 6.2552e-08,\n",
      "        2.8408e-07, 1.2375e-05, 1.0417e-05, 4.5753e-02, 8.0769e-06, 4.7003e-06,\n",
      "        3.5510e-06, 4.6321e-05, 3.4156e-05, 2.0751e-06, 7.9959e-07, 7.1106e-05,\n",
      "        1.9667e-07, 1.3203e-06, 2.4606e-08, 3.4253e-07, 4.4147e-07, 2.6501e-07,\n",
      "        1.3197e-06, 4.3548e-07, 6.6653e-05, 9.7601e-08, 1.9952e-08, 1.2618e-07,\n",
      "        4.1926e-09, 3.3232e-08, 4.1233e-09, 3.6052e-09, 2.4899e-08, 4.6032e-09,\n",
      "        1.7813e-08, 3.8692e-08, 1.1095e-09, 1.3021e-08, 1.1739e-07, 1.8670e-08,\n",
      "        1.6303e-08, 1.5933e-08, 3.8820e-07, 6.9612e-09, 3.8647e-09, 4.8942e-08,\n",
      "        4.3764e-08, 1.6111e-10, 1.2755e-09, 7.1850e-09, 1.4553e-09, 4.2678e-09,\n",
      "        4.8329e-08, 1.3352e-08, 1.4738e-08, 6.4235e-07, 3.2379e-08, 7.4673e-08,\n",
      "        1.2119e-06, 1.6435e-05, 2.9221e-05, 1.7153e-05, 1.3571e-06, 1.1036e-07,\n",
      "        3.8666e-07, 1.1823e-07, 1.9369e-06, 2.0143e-07, 7.7765e-08, 1.6911e-06,\n",
      "        5.7869e-08, 4.6857e-09, 3.1878e-09, 1.7127e-07, 3.9521e-08, 3.9169e-09,\n",
      "        1.0257e-06, 2.8826e-08, 2.9165e-08, 6.8581e-10, 2.1136e-09, 9.5344e-08,\n",
      "        8.6647e-09, 5.0514e-05, 2.3184e-05, 1.4931e-06, 4.8561e-05, 1.8537e-05,\n",
      "        4.4599e-08, 8.3801e-06, 1.2482e-06, 7.5608e-08, 1.9681e-08, 4.8057e-08,\n",
      "        4.3189e-08, 1.4821e-08, 6.3688e-07, 1.7671e-08, 7.5635e-08, 1.8750e-07,\n",
      "        3.9857e-07, 5.9662e-07, 2.2372e-07, 3.3383e-08, 1.9070e-09, 9.6602e-07,\n",
      "        3.1275e-07, 1.2987e-07, 2.2390e-07, 6.8922e-08, 1.2024e-07, 2.2185e-07,\n",
      "        8.1800e-08, 5.2933e-09, 1.0199e-09, 2.9616e-06, 1.3426e-05, 4.0917e-08,\n",
      "        1.6222e-08, 1.5809e-07, 1.7964e-09, 3.7843e-09, 4.3844e-08, 1.1114e-08,\n",
      "        1.7407e-09, 3.8219e-09, 3.0058e-08, 4.9538e-08, 3.1182e-08, 4.2190e-08,\n",
      "        3.9432e-08, 5.5536e-10, 4.4382e-09, 1.0674e-07, 7.9867e-09, 5.7643e-08,\n",
      "        6.8495e-09, 4.4752e-08, 5.6072e-09, 1.0497e-07, 4.4265e-06, 9.1010e-09,\n",
      "        4.5264e-08, 1.2857e-08, 9.6470e-09, 3.8991e-08, 1.8022e-08, 2.0314e-07,\n",
      "        7.6224e-08, 2.8772e-08, 4.1515e-08, 1.8686e-08, 9.5525e-09, 2.2949e-07,\n",
      "        1.0803e-08, 6.8947e-07, 1.4187e-06, 4.8974e-07, 4.2157e-09, 4.9288e-08,\n",
      "        1.8710e-08, 5.2137e-08, 1.6487e-07, 3.8188e-06, 3.5525e-08, 6.2990e-08,\n",
      "        1.4916e-08, 6.9043e-07, 3.7365e-07, 8.1306e-08, 1.1212e-07, 1.2056e-07,\n",
      "        1.3665e-07, 6.9041e-08, 4.5310e-08, 1.9412e-08, 6.5248e-08, 4.2940e-08,\n",
      "        8.9355e-07, 1.3681e-08, 1.4032e-08, 5.4683e-08, 3.2634e-08, 6.2838e-07,\n",
      "        6.9710e-08, 4.5180e-07, 4.0159e-08, 3.5897e-08, 1.1640e-07, 4.5406e-08,\n",
      "        2.2978e-06, 2.2968e-05, 2.1077e-08, 1.4575e-08, 6.0548e-09, 3.1755e-09,\n",
      "        2.7594e-08, 1.9556e-07, 2.0456e-07, 6.7905e-08, 1.0043e-06, 3.3708e-07,\n",
      "        2.0691e-08, 5.7164e-08, 3.1960e-08, 6.5392e-08, 4.2804e-07, 8.7879e-07,\n",
      "        3.6323e-08, 5.8888e-09, 1.1183e-08, 8.9813e-08, 1.0511e-07, 1.9758e-08,\n",
      "        1.6400e-08, 3.5244e-08, 3.4495e-08, 2.0751e-07, 2.2501e-08, 3.3327e-07,\n",
      "        5.7653e-08, 1.9439e-08, 6.3455e-08, 2.3427e-08, 8.3302e-07, 3.8414e-08,\n",
      "        5.1430e-09, 8.1902e-08, 2.4027e-09, 2.5505e-07, 2.6952e-07, 2.5224e-08,\n",
      "        1.0031e-07, 2.7908e-08, 1.0216e-08, 1.6101e-07, 8.6127e-07, 1.3511e-08,\n",
      "        6.9958e-08, 1.8372e-07, 5.0400e-08, 6.4440e-09, 2.2783e-08, 1.3105e-07,\n",
      "        1.5272e-08, 6.6869e-09, 1.3206e-07, 5.1829e-08, 9.7422e-09, 2.2224e-06,\n",
      "        1.8445e-06, 1.4807e-07, 3.8161e-08, 2.9394e-08, 5.2608e-08, 3.5367e-08,\n",
      "        6.7184e-07, 1.6892e-07, 2.7721e-08, 2.1015e-08, 7.1155e-09, 3.1867e-09,\n",
      "        6.3566e-07, 8.5882e-08, 1.3994e-08, 1.1256e-06, 3.4747e-09, 4.2733e-06,\n",
      "        4.4594e-09, 1.0644e-07, 7.6564e-08, 1.2817e-07, 5.4634e-07, 7.0479e-08,\n",
      "        1.0126e-07, 4.1240e-09, 6.8692e-09, 1.5285e-07, 1.1539e-09, 1.2175e-07,\n",
      "        2.3883e-07, 4.0903e-08, 3.7285e-08, 5.0836e-08, 5.4075e-08, 3.9270e-08,\n",
      "        8.2493e-09, 3.5183e-07, 2.7117e-07, 1.0010e-07, 7.7955e-09, 4.3325e-08,\n",
      "        1.8377e-09, 4.1355e-09, 3.8112e-08, 8.3479e-08, 4.8975e-07, 3.2350e-08,\n",
      "        3.2581e-08, 1.6079e-09, 1.0673e-07, 9.7948e-08, 1.1158e-06, 9.8420e-08,\n",
      "        1.0254e-08, 1.2991e-07, 2.2682e-07, 2.0077e-08, 3.3397e-07, 4.1824e-08,\n",
      "        9.8195e-09, 2.2020e-08, 2.1698e-07, 1.4505e-07, 7.1213e-08, 2.6099e-07,\n",
      "        1.5021e-07, 3.4766e-06, 1.2570e-08, 3.8712e-08, 5.8540e-09, 1.4547e-08,\n",
      "        2.4685e-07, 2.0828e-08, 7.2565e-07, 5.0589e-08, 2.3091e-08, 7.0352e-09,\n",
      "        1.6852e-07, 3.0241e-08, 1.5120e-08, 1.0949e-08, 1.4973e-09, 9.2629e-09,\n",
      "        6.4041e-07, 5.8662e-08, 1.2368e-07, 4.1383e-08, 2.4110e-07, 4.8010e-07,\n",
      "        1.2644e-09, 9.9347e-08, 6.1049e-08, 2.9989e-07, 1.8609e-08, 2.6847e-08,\n",
      "        7.9455e-08, 6.6688e-08, 1.0533e-07, 4.8775e-07, 1.9920e-06, 5.3178e-08,\n",
      "        8.9093e-09, 4.9173e-08, 1.6455e-07, 7.8707e-08, 9.6218e-09, 1.0347e-07,\n",
      "        3.9173e-07, 1.0922e-06, 9.9406e-08, 1.7501e-07, 3.8472e-09, 5.9330e-08,\n",
      "        9.7181e-08, 8.6531e-08, 4.9112e-08, 4.9306e-08, 1.4626e-07, 6.3884e-07,\n",
      "        2.8941e-09, 4.5515e-08, 1.2413e-07, 1.1408e-08, 4.7240e-07, 1.4246e-08,\n",
      "        1.4248e-07, 1.4648e-08, 1.4426e-08, 7.5724e-08, 3.8128e-08, 5.6943e-07,\n",
      "        4.6431e-08, 1.6758e-07, 3.9160e-08, 1.7813e-08, 1.3464e-07, 9.5358e-08,\n",
      "        5.6186e-08, 9.4821e-09, 3.0314e-08, 1.0802e-08, 1.7497e-07, 2.6956e-07,\n",
      "        3.0364e-08, 1.3100e-07, 1.8943e-09, 3.0386e-08, 5.5207e-07, 5.8822e-08,\n",
      "        1.0079e-07, 6.1554e-07, 2.5411e-07, 9.3760e-08, 2.4213e-06, 8.3388e-08,\n",
      "        3.3974e-08, 9.9482e-08, 2.5670e-07, 1.5562e-07, 4.4525e-09, 5.0270e-09,\n",
      "        1.2250e-07, 1.1895e-08, 1.0204e-07, 1.2525e-09, 9.1031e-08, 3.4094e-08,\n",
      "        1.3415e-08, 2.8809e-08, 4.2425e-08, 3.3126e-08, 6.8699e-08, 4.5778e-08,\n",
      "        3.5857e-07, 3.3667e-07, 2.0707e-08, 6.8147e-08, 1.5075e-06, 1.9400e-08,\n",
      "        1.8309e-09, 1.9017e-07, 5.7636e-09, 3.5434e-09, 2.8860e-07, 1.8676e-08,\n",
      "        5.3227e-08, 2.3498e-08, 3.5835e-08, 5.8415e-08, 4.1506e-08, 1.5948e-08,\n",
      "        3.0443e-07, 7.0300e-09, 5.4922e-07, 1.0548e-07, 6.8452e-08, 8.7136e-07,\n",
      "        2.6842e-07, 3.9101e-08, 7.4620e-07, 4.8165e-07, 4.1447e-09, 4.2622e-08,\n",
      "        4.5918e-10, 5.4388e-09, 6.4265e-07, 1.9697e-08, 7.9124e-09, 6.1583e-06,\n",
      "        7.9291e-09, 2.7212e-08, 3.0804e-09, 1.4850e-07, 1.0336e-08, 6.1572e-08,\n",
      "        1.1373e-07, 4.0316e-09, 3.6594e-07, 3.0231e-08, 2.4034e-07, 3.0384e-08,\n",
      "        2.1521e-08, 1.0775e-08, 2.1314e-07, 1.7550e-07, 3.9164e-07, 2.8834e-07,\n",
      "        1.6746e-07, 1.6674e-07, 2.5735e-07, 3.0095e-07, 3.4690e-08, 1.9335e-08,\n",
      "        1.6524e-05, 1.2666e-07, 1.2384e-07, 1.2832e-07, 1.5777e-07, 7.3495e-07,\n",
      "        2.0261e-08, 1.9262e-07, 1.4838e-08, 9.6571e-08, 5.9067e-08, 4.6632e-08,\n",
      "        6.2713e-07, 3.2282e-07, 1.1002e-07, 3.0725e-07, 3.4523e-07, 5.2771e-08,\n",
      "        1.0151e-07, 2.9580e-08, 8.0421e-09, 4.6297e-08, 4.7886e-08, 9.1367e-09,\n",
      "        5.3363e-09, 4.9734e-08, 3.3670e-07, 1.4789e-07, 2.2299e-07, 5.6245e-08,\n",
      "        2.9747e-07, 1.5442e-07, 1.0017e-07, 7.8586e-08, 1.5717e-08, 1.5123e-07,\n",
      "        2.8017e-07, 3.7914e-08, 8.2572e-08, 4.6543e-08, 1.8414e-08, 8.0664e-07,\n",
      "        3.3823e-08, 1.6269e-07, 5.4251e-10, 4.0951e-08, 4.7638e-08, 6.7055e-09,\n",
      "        2.4043e-07, 4.4549e-05, 1.8777e-07, 5.0038e-08, 1.9936e-08, 9.6115e-08,\n",
      "        3.2692e-07, 1.0607e-06, 2.0506e-08, 1.7775e-07, 5.2924e-08, 2.8164e-08,\n",
      "        2.1009e-08, 1.3275e-07, 7.7362e-08, 3.0877e-08, 3.7842e-08, 1.0877e-08,\n",
      "        3.8656e-08, 2.6546e-07, 2.8905e-07, 1.8199e-07, 4.7048e-08, 4.3829e-07,\n",
      "        1.2603e-07, 1.8863e-08, 3.1505e-08, 2.4164e-07, 2.7383e-08, 2.0091e-08,\n",
      "        5.2270e-08, 2.6621e-08, 1.3460e-07, 3.5969e-07, 4.8343e-07, 5.0256e-08,\n",
      "        6.1656e-08, 2.1765e-07, 4.8667e-07, 1.1960e-07, 1.8928e-07, 2.3989e-07,\n",
      "        1.8091e-08, 4.4516e-08, 1.7338e-08, 2.7734e-08, 2.5819e-07, 3.4705e-07,\n",
      "        1.1465e-05, 1.6594e-08, 4.4105e-08, 3.9391e-08, 1.1660e-08, 2.0015e-09,\n",
      "        2.6693e-08, 8.1286e-08, 7.0877e-09, 8.8738e-07, 6.6537e-08, 1.2551e-08,\n",
      "        3.5646e-08, 2.6889e-08, 1.9886e-08, 5.6372e-08, 3.3148e-08, 9.9487e-08,\n",
      "        3.7086e-07, 3.6744e-08, 5.6302e-08, 2.9907e-08, 6.6878e-09, 3.0150e-08,\n",
      "        8.7878e-06, 1.4142e-08, 1.8047e-07, 1.6546e-07, 3.9988e-07, 2.6022e-08,\n",
      "        2.9437e-07, 3.3218e-08, 1.0731e-08, 1.1875e-07, 7.8077e-09, 1.0533e-08,\n",
      "        2.0368e-08, 8.1599e-08, 4.0596e-08, 2.0780e-08, 3.4279e-08, 8.7408e-08,\n",
      "        4.4256e-08, 1.7798e-09, 9.9956e-07, 1.4500e-06, 2.3129e-07, 7.0463e-08,\n",
      "        2.6234e-08, 9.0374e-08, 3.3703e-07, 1.8924e-08, 2.7502e-07, 1.1875e-08,\n",
      "        8.0118e-09, 9.9674e-08, 9.7456e-09, 4.3983e-08, 1.8532e-07, 1.6864e-07,\n",
      "        3.0605e-07, 6.3739e-08, 4.1832e-08, 1.6963e-07, 6.6313e-08, 2.9990e-09,\n",
      "        6.1167e-08, 1.5901e-08, 7.7053e-09, 5.5688e-08, 8.9480e-10, 4.0636e-08,\n",
      "        5.9931e-09, 6.4041e-09, 2.7640e-09, 4.3533e-09, 4.0892e-09, 3.7811e-06,\n",
      "        8.3826e-09, 9.9930e-09, 4.5139e-08, 6.1673e-10, 5.1868e-09, 1.5333e-07,\n",
      "        5.4814e-08, 1.7694e-07, 2.5677e-07, 3.3827e-07, 1.1170e-08, 1.5784e-09,\n",
      "        4.9137e-08, 5.7166e-07, 8.2899e-09, 1.7344e-09, 2.9642e-09, 2.7439e-08,\n",
      "        5.0204e-07, 1.2235e-07, 5.6038e-09, 8.8459e-09, 4.5322e-09, 2.0751e-08,\n",
      "        2.7102e-08, 4.7874e-09, 1.5054e-08, 5.0198e-08, 6.2460e-08, 6.3068e-09,\n",
      "        1.8961e-08, 1.6234e-07, 5.4849e-09, 8.9252e-09, 4.3098e-10, 7.0278e-09,\n",
      "        4.8261e-08, 9.5612e-10, 5.9642e-08, 3.1656e-08, 1.2140e-07, 3.4457e-07,\n",
      "        2.4260e-08, 2.1168e-09, 6.6485e-08, 6.8931e-07, 1.4292e-08, 1.6209e-07,\n",
      "        2.5790e-08, 3.4866e-08, 4.0267e-08, 8.5099e-08, 2.3616e-07, 1.8334e-08,\n",
      "        3.2955e-08, 1.5061e-08, 8.1571e-09, 1.5166e-07, 2.3173e-09, 1.7184e-08,\n",
      "        2.0623e-08, 6.3171e-08, 4.4877e-08, 2.4538e-10, 1.2113e-08, 3.9385e-08,\n",
      "        1.2425e-08, 4.1200e-09, 1.3280e-07, 9.3502e-07], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "assert isinstance(input_tensor, t.Tensor)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if t.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "\n",
    "with t.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = t.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-06 07:27:38--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt.4’\n",
      "\n",
      "imagenet_classes.tx 100%[===================>]  10,23K  --.-KB/s    in 0,004s  \n",
      "\n",
      "2023-12-06 07:27:38 (2,46 MB/s) - ‘imagenet_classes.txt.4’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.8845872282981873\n",
      "Arctic fox 0.04575274884700775\n",
      "white wolf 0.04435020312666893\n",
      "Pomeranian 0.00561820063740015\n",
      "Great Pyrenees 0.004655089695006609\n"
     ]
    }
   ],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = t.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, param in model.named_parameters():\n",
    "    param.requires_grad_(False)\n",
    "assert all(not param.requires_grad for _, param in model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device=t.device(\"cuda\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate artificial image that maximizes a neuron in `layer1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 943.60it/s]\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "lr = 1e-3\n",
    "N = 1_0_000\n",
    "act_inds = (0, 0, 22, 20,)\n",
    "x = t.randn_like(input_batch)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x.requires_grad_(True)\n",
    "    acts = model.layer1(model.maxpool(model.relu(model.bn1(model.conv1(x)))))\n",
    "    act = acts[act_inds]\n",
    "    act.backward()\n",
    "    with t.no_grad():\n",
    "        x = x + lr * cast(t.Tensor, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m x\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m acts \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayer1(model\u001b[39m.\u001b[39mmaxpool(model\u001b[39m.\u001b[39mrelu(model\u001b[39m.\u001b[39mbn1(model\u001b[39m.\u001b[39mconv1(x)))))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m acts\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# act.backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m t\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[1;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "lr = 1e2\n",
    "N = 1_0_000\n",
    "act_inds = (0, 0, 0, 0,)\n",
    "x = t.randn_like(input_batch)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x.requires_grad_(True)\n",
    "    acts = model.layer1(model.maxpool(model.relu(model.bn1(model.conv1(x)))))\n",
    "    acts.backward()\n",
    "    \n",
    "    for act in acts.flatten():\n",
    "        \n",
    "    \n",
    "    with t.no_grad():\n",
    "        x = x + lr * cast(t.Tensor, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(x[0,0].detach().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:16<00:00, 618.17it/s]\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "lr = 1e2\n",
    "N = 1_0000\n",
    "act_inds = (0, 0, 22, 20,)\n",
    "x = t.randn_like(input_batch)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x.requires_grad_(True)\n",
    "    acts = model.layer2(model.layer1(model.maxpool(model.relu(model.bn1(model.conv1(x))))))\n",
    "    act = acts[act_inds]\n",
    "    act.backward()\n",
    "    with t.no_grad():\n",
    "        x = x + lr * cast(t.Tensor, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(x[0,0].detach().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:27<00:00, 357.67it/s]\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "lr = 1e2\n",
    "N = 1_0000\n",
    "act_inds = (0, 0, 10, 10,)\n",
    "x = t.randn_like(input_batch)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x.requires_grad_(True)\n",
    "    acts = model.layer3(model.layer2(model.layer1(model.maxpool(model.relu(model.bn1(model.conv1(x)))))))\n",
    "    act = acts[act_inds]\n",
    "    act.backward()\n",
    "    with t.no_grad():\n",
    "        x = x + lr * cast(t.Tensor, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(x[0,0].detach().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:37<00:00, 270.17it/s]\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "lr = 1e2\n",
    "N = 1_0000\n",
    "act_inds = (0, 0, 5, 5,)\n",
    "x = t.randn_like(input_batch)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x.requires_grad_(True)\n",
    "    acts = model.layer4(model.layer3(model.layer2(model.layer1(model.maxpool(model.relu(model.bn1(model.conv1(x))))))))\n",
    "    act = acts[act_inds]\n",
    "    act.backward()\n",
    "    with t.no_grad():\n",
    "        x = x + lr * cast(t.Tensor, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(x[0,0].detach().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m seed(\u001b[39m42\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m1e2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/8/cnn_resnet_interp.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m1_0000\u001b[39m\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/solutions/8/src/utils.py:42\u001b[0m, in \u001b[0;36mseed\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseed\u001b[39m(val: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     random\u001b[39m.\u001b[39mseed(val)\n\u001b[0;32m---> 42\u001b[0m     t\u001b[39m.\u001b[39;49mmanual_seed(val)\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmanual_seed_all(seed)\n\u001b[1;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmps\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/cuda/random.py:124\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    121\u001b[0m         default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    122\u001b[0m         default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 124\u001b[0m _lazy_call(cb, seed_all\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy_call\u001b[39m(\u001b[39mcallable\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    228\u001b[0m     \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 229\u001b[0m         \u001b[39mcallable\u001b[39;49m()\n\u001b[1;32m    230\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         \u001b[39m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         \u001b[39m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    233\u001b[0m         \u001b[39m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m         \u001b[39mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/cuda/random.py:122\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[1;32m    121\u001b[0m     default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 122\u001b[0m     default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "lr = 1e2\n",
    "N = 1_0000\n",
    "act_inds = (0, 0)\n",
    "x = t.randn_like(input_batch)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x.requires_grad_(True)\n",
    "    acts = model.layer4(model.layer3(model.layer2(model.layer1(model.maxpool(model.relu(model.bn1(model.conv1(x))))))))\n",
    "    acts = model.fc(model.avgpool(acts).flatten(1))\n",
    "    act = acts[act_inds]\n",
    "    act.backward()\n",
    "    with t.no_grad():\n",
    "        x = x + lr * cast(t.Tensor, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(x[0,0].detach().cpu().numpy()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
