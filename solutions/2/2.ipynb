{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import product\n",
    "import math\n",
    "from operator import add, eq\n",
    "import pathlib\n",
    "from typing import NamedTuple, Self\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "#    Config    #\n",
    "################\n",
    "\n",
    "REFERENCE_MODEL_SIZE = 6\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Config:\n",
    "    \"\"\"Experiment configuration\"\"\"\n",
    "    model_size: int\n",
    "    \"\"\"Number of (hidden) channels in each of the two convolutional layers\n",
    "    in terms of power of sqrt(2) (rounded if necessary).\n",
    "    \"\"\"\n",
    "    dataset_size: int\n",
    "    \"\"\"Fraction of training data used for training\n",
    "    in terms of the powers of (1/2).\n",
    "    \"\"\"\n",
    "    # KW\n",
    "    fc_dim: int = field(default=64, kw_only=True)\n",
    "    \"\"\"Fully connected layer dimension\"\"\"\n",
    "    kernel_size: int = field(default=3, kw_only=True)\n",
    "    \"\"\"Convolution kernel size\"\"\"\n",
    "    n_epochs: int = field(default=1, kw_only=True)\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        assert 0 < self.model_size, f\"model_size={self.model_size}\"\n",
    "        assert 0 <= self.dataset_size, f\"dataset_size={self.dataset_size}\"\n",
    "        assert 0 < self.n_epochs, f\"n_epochs={self.n_epochs}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def default(cls) -> Self:\n",
    "        return cls(\n",
    "            model_size=1,\n",
    "            dataset_size=0\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def n_channels(self) -> int:\n",
    "        return REFERENCE_MODEL_SIZE * int(math.sqrt(2) ** self.model_size)\n",
    "    \n",
    "    @property\n",
    "    def dataset_fraction(self) -> float:\n",
    "        return 0.5 ** self.dataset_size\n",
    "        \n",
    "cfg = Config.default()\n",
    "\n",
    "#################\n",
    "#    Dataset    #\n",
    "#################\n",
    "\n",
    "\n",
    "DATA_PATH = pathlib.Path(\"../data\")\n",
    "assert DATA_PATH.exists()\n",
    "\n",
    "def preprocess_batch(batch: t.Tensor) -> t.Tensor:\n",
    "    assert batch.ndim == 3\n",
    "    assert eq(*batch.shape[1:])\n",
    "    batch_dim, im_dim = batch.shape[:2]\n",
    "    processed_batch = batch.to(dtype=t.float32).unsqueeze(-1).reshape(batch_dim, 1, im_dim, im_dim)\n",
    "    return (processed_batch - processed_batch.mean()) / processed_batch.std()\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Dataset:\n",
    "    train_x: t.Tensor\n",
    "    train_y: t.Tensor\n",
    "    test_x: t.Tensor\n",
    "    test_y: t.Tensor\n",
    "    \n",
    "    @classmethod\n",
    "    def make(cls, cfg: Config = Config.default()) -> Self:\n",
    "        train = MNIST(str(DATA_PATH), train=True, download=True)\n",
    "        test = MNIST(str(DATA_PATH), train=False, download=True)\n",
    "        dataset_size = int(cfg.dataset_fraction * len(train.data))\n",
    "        return cls(\n",
    "            train_x=preprocess_batch(train.data)[:dataset_size],\n",
    "            train_y=train.targets[:dataset_size],\n",
    "            test_x=preprocess_batch(test.data),\n",
    "            test_y=test.targets\n",
    "        )\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert len(self.train_x) == len(self.train_y)\n",
    "        assert len(self.test_x) == len(self.test_y)\n",
    "        assert self.train_x.ndim == self.test_x.ndim == 4\n",
    "        assert self.train_y.ndim == self.test_y.ndim == 1\n",
    "        assert self.train_x.shape[1:] == self.test_x.shape[1:]\n",
    "        \n",
    "        \n",
    "ds = Dataset.make()\n",
    "ds.train_x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training accuracy: 9.9217%\n",
      "Initial test accuracy: 10.1%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, cfg: Config = Config.default()) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=cfg.n_channels, kernel_size=cfg.kernel_size, padding=1\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=cfg.n_channels, out_channels=cfg.n_channels, kernel_size=cfg.kernel_size, padding=1\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(cfg.n_channels * 7 * 7, cfg.fc_dim)\n",
    "        self.fc2 = nn.Linear(cfg.fc_dim, 10)\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "####################\n",
    "#    Evaluation    #\n",
    "####################\n",
    "\n",
    "def assert_valid_x(x: t.Tensor) -> None:\n",
    "    assert x.ndim == 4\n",
    "    assert x.shape[1] == 1\n",
    "    assert eq(*x.shape[2:])\n",
    "    \n",
    "def assert_valid_y(y: t.Tensor) -> None:\n",
    "    assert y.ndim == 1\n",
    "    assert y.min() >= 0\n",
    "    assert y.max() <= 9\n",
    "\n",
    "def acc_fn(\n",
    "    logits: t.Tensor, y: t.Tensor, *, as_pct: bool = True, pct_round_digits: int = 2\n",
    ") -> float:\n",
    "    # assert_valid_y(y)\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = (preds == y).to(dtype=t.float).mean().item()\n",
    "    if as_pct:\n",
    "        acc = round(100 * acc, pct_round_digits + 2)\n",
    "    return acc\n",
    "\n",
    "model = CNN()\n",
    "train_logits = model(ds.train_x)\n",
    "test_logits = model(ds.test_x)\n",
    "\n",
    "print(f\"Initial training accuracy: {acc_fn(train_logits, ds.train_y)}%\")\n",
    "print(f\"Initial test accuracy: {acc_fn(test_logits, ds.test_y)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]: train_loss=2.2226, test_loss=2.2239, train_acc=23.8333%, test_acc=23.84%\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True, slots=True)\n",
    "class TrainingResult:\n",
    "    cfg: Config\n",
    "    ds: Dataset\n",
    "    model: CNN\n",
    "    train_loss: list[float] = field(default_factory=list, kw_only=True)\n",
    "    test_loss: list[float] = field(default_factory=list, kw_only=True)\n",
    "    train_acc: list[float] = field(default_factory=list, kw_only=True)\n",
    "    test_acc: list[float] = field(default_factory=list, kw_only=True)\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        assert len(self.train_loss) == len(self.test_loss) == len(self.train_acc) == len(self.test_acc)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.train_acc)\n",
    "        \n",
    "    def append(\n",
    "        self,\n",
    "        *,\n",
    "        train_loss: float,\n",
    "        test_loss: float,\n",
    "        train_acc: float,\n",
    "        test_acc: float\n",
    "    ) -> None:\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.test_loss.append(test_loss)\n",
    "        self.train_acc.append(train_acc)\n",
    "        self.test_acc.append(test_acc)\n",
    "    \n",
    "    def log_last(self, *, loss_digits: int = 4, acc_digits: int = 2) -> None:\n",
    "        if len(self) == 0:\n",
    "            print(f\"[0]\")\n",
    "            return\n",
    "        train_loss = round(self.train_loss[-1], loss_digits)\n",
    "        test_loss = round(self.test_loss[-1], loss_digits)\n",
    "        train_acc = round(100 * self.train_acc[-1], acc_digits + 2) if 0 <= self.train_acc[-1] <= 1 else self.train_acc[-1]\n",
    "        test_acc = round(100 * self.test_acc[-1], acc_digits + 2) if 0 <= self.test_acc[-1] <= 1 else self.test_acc[-1]\n",
    "        print(f\"[{len(self)}]: {train_loss=}, {test_loss=}, train_acc={train_acc}%, test_acc={test_acc}%\")    \n",
    "\n",
    "def train(cfg: Config) -> TrainingResult:\n",
    "    ds = Dataset.make(cfg)\n",
    "    model = CNN(cfg)\n",
    "    tr = TrainingResult(cfg, ds, model)\n",
    "    optimizer = t.optim.AdamW(model.parameters()) #TODO: change/tweak/experiment with hyperparams?\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch_i in range(cfg.n_epochs):\n",
    "        # Training \n",
    "        train_logits = model(ds.train_x)\n",
    "        train_loss = loss_fn(train_logits, ds.train_y)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Measure\n",
    "        with t.inference_mode():\n",
    "            train_logits = model(ds.train_x)\n",
    "            train_loss = loss_fn(train_logits, ds.train_y).item()\n",
    "            train_acc = acc_fn(train_logits, ds.train_y)\n",
    "            test_logits = model(ds.test_x)\n",
    "            test_loss = loss_fn(test_logits, ds.test_y).item()\n",
    "            test_acc = acc_fn(test_logits, ds.test_y)\n",
    "            tr.append(train_loss=train_loss, test_loss=test_loss, train_acc=train_acc, test_acc=test_acc)\n",
    "        \n",
    "    \n",
    "    return tr\n",
    "\n",
    "cfg = Config(1, 0, n_epochs=5)\n",
    "tr = train(cfg)\n",
    "tr.log_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=90\n",
      "[0/90] model: 1, dataset: 0\n",
      "[1/90] model: 1, dataset: 1\n",
      "[2/90] model: 1, dataset: 2\n",
      "[3/90] model: 1, dataset: 3\n",
      "[4/90] model: 1, dataset: 4\n",
      "[5/90] model: 1, dataset: 5\n",
      "[6/90] model: 1, dataset: 6\n",
      "[7/90] model: 1, dataset: 7\n",
      "[8/90] model: 1, dataset: 8\n",
      "[9/90] model: 1, dataset: 9\n",
      "[10/90] model: 2, dataset: 0\n",
      "[11/90] model: 2, dataset: 1\n",
      "[12/90] model: 2, dataset: 2\n",
      "[13/90] model: 2, dataset: 3\n",
      "[14/90] model: 2, dataset: 4\n",
      "[15/90] model: 2, dataset: 5\n",
      "[16/90] model: 2, dataset: 6\n",
      "[17/90] model: 2, dataset: 7\n",
      "[18/90] model: 2, dataset: 8\n",
      "[19/90] model: 2, dataset: 9\n",
      "[20/90] model: 3, dataset: 0\n",
      "[21/90] model: 3, dataset: 1\n",
      "[22/90] model: 3, dataset: 2\n",
      "[23/90] model: 3, dataset: 3\n",
      "[24/90] model: 3, dataset: 4\n",
      "[25/90] model: 3, dataset: 5\n",
      "[26/90] model: 3, dataset: 6\n",
      "[27/90] model: 3, dataset: 7\n",
      "[28/90] model: 3, dataset: 8\n",
      "[29/90] model: 3, dataset: 9\n",
      "[30/90] model: 4, dataset: 0\n",
      "[31/90] model: 4, dataset: 1\n",
      "[32/90] model: 4, dataset: 2\n",
      "[33/90] model: 4, dataset: 3\n",
      "[34/90] model: 4, dataset: 4\n",
      "[35/90] model: 4, dataset: 5\n",
      "[36/90] model: 4, dataset: 6\n",
      "[37/90] model: 4, dataset: 7\n",
      "[38/90] model: 4, dataset: 8\n",
      "[39/90] model: 4, dataset: 9\n",
      "[40/90] model: 5, dataset: 0\n",
      "[41/90] model: 5, dataset: 1\n",
      "[42/90] model: 5, dataset: 2\n",
      "[43/90] model: 5, dataset: 3\n",
      "[44/90] model: 5, dataset: 4\n",
      "[45/90] model: 5, dataset: 5\n",
      "[46/90] model: 5, dataset: 6\n",
      "[47/90] model: 5, dataset: 7\n",
      "[48/90] model: 5, dataset: 8\n",
      "[49/90] model: 5, dataset: 9\n",
      "[50/90] model: 6, dataset: 0\n",
      "[51/90] model: 6, dataset: 1\n",
      "[52/90] model: 6, dataset: 2\n",
      "[53/90] model: 6, dataset: 3\n",
      "[54/90] model: 6, dataset: 4\n",
      "[55/90] model: 6, dataset: 5\n",
      "[56/90] model: 6, dataset: 6\n",
      "[57/90] model: 6, dataset: 7\n",
      "[58/90] model: 6, dataset: 8\n",
      "[59/90] model: 6, dataset: 9\n",
      "[60/90] model: 7, dataset: 0\n",
      "[61/90] model: 7, dataset: 1\n",
      "[62/90] model: 7, dataset: 2\n",
      "[63/90] model: 7, dataset: 3\n",
      "[64/90] model: 7, dataset: 4\n",
      "[65/90] model: 7, dataset: 5\n",
      "[66/90] model: 7, dataset: 6\n",
      "[67/90] model: 7, dataset: 7\n",
      "[68/90] model: 7, dataset: 8\n",
      "[69/90] model: 7, dataset: 9\n",
      "[70/90] model: 8, dataset: 0\n"
     ]
    }
   ],
   "source": [
    "MODEL_SIZES: list[int] = list(range(1, 10))\n",
    "DATASET_SIZES: list[int] = list(range(0, 10))\n",
    "\n",
    "PARAMS = list(product(MODEL_SIZES, DATASET_SIZES))\n",
    "N = len(PARAMS)\n",
    "print(f\"{N=}\")\n",
    "\n",
    "trs: list[TrainingResult] = []\n",
    "\n",
    "for param_i, (model_size, dataset_size) in enumerate(PARAMS):\n",
    "    print(f\"[{param_i}/{N}] model: {model_size}, dataset: {dataset_size}\")\n",
    "    cfg = Config(model_size, dataset_size)\n",
    "    tr = train(cfg)\n",
    "    trs.append(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_SIZES = [3, 6, 9, 12, 15, 18]\n",
      "DATASET_SIZES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "MODEL_SIZES: list[int] = np.arange(3, 20, step=3).tolist()\n",
    "DATASET_SIZES: list[float] = np.linspace(0.1, 1, 10).round(1).tolist()\n",
    "print(f\"{MODEL_SIZES = }\\n{DATASET_SIZES = }\")\n",
    "\n",
    "class Result(NamedTuple):\n",
    "    cfg: Config\n",
    "    cnn: CNN\n",
    "    train_loss: float\n",
    "    test_loss: float\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "\n",
    "def acc_fn(logits: t.Tensor, y: t.Tensor) -> float:\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = (preds == y).to(t.float).mean().item()\n",
    "    return acc\n",
    "\n",
    "def train(model_size: int, dataset_size: float) -> Result:\n",
    "    # Setup\n",
    "    cfg = Config(model_size=model_size, dataset_size=dataset_size)\n",
    "    cnn = CNN(cfg)\n",
    "    ds = Dataset.make(cfg)    \n",
    "    optimizer = t.optim.AdamW(cnn.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Training\n",
    "    train_logits = cnn(ds.train_x)\n",
    "    train_loss = loss_fn(train_logits, ds.train_y)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    # Eval\n",
    "    with t.inference_mode():\n",
    "        train_logits = cnn(ds.train_x)\n",
    "        train_loss = loss_fn(train_logits, ds.train_y).item()\n",
    "        test_logits = cnn(ds.test_x)\n",
    "        test_loss = loss_fn(test_logits, ds.test_y).item()\n",
    "        train_acc = acc_fn(train_logits, ds.train_y)\n",
    "        test_acc = acc_fn(test_logits, ds.test_y)\n",
    "    return Result(\n",
    "        cfg=cfg,\n",
    "        cnn=cnn,\n",
    "        train_loss=train_loss,\n",
    "        test_loss=test_loss,\n",
    "        train_acc=train_acc,\n",
    "        test_acc=test_acc\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/60]: model_size=3, dataset_size=0.1\n",
      "[1/60]: model_size=3, dataset_size=0.2\n",
      "[2/60]: model_size=3, dataset_size=0.3\n",
      "[3/60]: model_size=3, dataset_size=0.4\n",
      "[4/60]: model_size=3, dataset_size=0.5\n",
      "[5/60]: model_size=3, dataset_size=0.6\n",
      "[6/60]: model_size=3, dataset_size=0.7\n",
      "[7/60]: model_size=3, dataset_size=0.8\n",
      "[8/60]: model_size=3, dataset_size=0.9\n",
      "[9/60]: model_size=3, dataset_size=1.0\n",
      "[10/60]: model_size=6, dataset_size=0.1\n",
      "[11/60]: model_size=6, dataset_size=0.2\n",
      "[12/60]: model_size=6, dataset_size=0.3\n",
      "[13/60]: model_size=6, dataset_size=0.4\n",
      "[14/60]: model_size=6, dataset_size=0.5\n",
      "[15/60]: model_size=6, dataset_size=0.6\n",
      "[16/60]: model_size=6, dataset_size=0.7\n",
      "[17/60]: model_size=6, dataset_size=0.8\n",
      "[18/60]: model_size=6, dataset_size=0.9\n",
      "[19/60]: model_size=6, dataset_size=1.0\n",
      "[20/60]: model_size=9, dataset_size=0.1\n",
      "[21/60]: model_size=9, dataset_size=0.2\n",
      "[22/60]: model_size=9, dataset_size=0.3\n",
      "[23/60]: model_size=9, dataset_size=0.4\n",
      "[24/60]: model_size=9, dataset_size=0.5\n",
      "[25/60]: model_size=9, dataset_size=0.6\n",
      "[26/60]: model_size=9, dataset_size=0.7\n",
      "[27/60]: model_size=9, dataset_size=0.8\n",
      "[28/60]: model_size=9, dataset_size=0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (model_size, dataset_size) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(PARAMS):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mN\u001b[39m}\u001b[39;00m\u001b[39m]: \u001b[39m\u001b[39m{\u001b[39;00mmodel_size\u001b[39m=}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mdataset_size\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     result \u001b[39m=\u001b[39m train(model_size, dataset_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(train_logits, ds\u001b[39m.\u001b[39mtrain_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Eval\u001b[39;00m\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PARAMS = list(product(MODEL_SIZES, DATASET_SIZES))\n",
    "N = len(PARAMS)\n",
    "\n",
    "results: list[Result] = []\n",
    "for i, (model_size, dataset_size) in enumerate(PARAMS):\n",
    "    print(f\"[{i}/{N}]: {model_size=}, {dataset_size=}\")\n",
    "    result = train(model_size, dataset_size)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
