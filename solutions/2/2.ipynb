{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import itertools as it\n",
    "import math\n",
    "import random\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import CNN, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vary the model size and dataset size.\n",
    "\n",
    "For the **model size**, we multiply the width by powers of $\\sqrt2$, rounding down if necessary. The idea is to vary the amount of compute used per forward pass by powers of $2$.\n",
    "\n",
    "For the **dataset size**, we multiply the fraction of the full dataset used by powers of $2$, i.e. $1$, $\\frac12$, $\\frac14$, and so on.\n",
    "\n",
    "To reduce noise, use a few random seeds and always use the full validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_SIZES = [6, 8, 11, 16, 23, 33, 47, 67, 95, 135]\n",
      "DATASET_FRACTIONS = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125]\n",
      "SEEDS = [654, 114, 25, 759, 281, 250, 228, 142, 754, 104, 692, 758, 913, 558, 89, 604, 432, 32, 30, 95, 223, 238, 517, 616, 27, 574, 203, 733, 665, 718, 986, 429, 225, 459, 603, 284, 828, 890, 6, 777, 825, 163, 714, 983, 348, 964, 159, 220, 781, 344, 990, 94, 389, 99, 367, 867, 352, 618, 270, 826, 44, 747, 470, 549, 127, 387, 80, 565, 300, 849, 643, 633, 906, 882, 370, 591, 196, 721, 71, 46, 677, 233, 791, 296, 81, 875, 978, 887, 103, 947, 954, 464, 650, 854, 373, 166, 379, 363, 214, 686]\n"
     ]
    }
   ],
   "source": [
    "# Model size\n",
    "REFERENCE_MODEL_SIZE = 6\n",
    "N_MODEL_SIZES = 10\n",
    "MODEL_SIZES = (REFERENCE_MODEL_SIZE * math.sqrt(2) ** t.arange(N_MODEL_SIZES)).to(dtype=t.int64).tolist()\n",
    "\n",
    "# Dataset size\n",
    "N_DATASET_SIZES = 10\n",
    "DATASET_FRACTIONS =  (1 / (2 ** t.arange(N_DATASET_SIZES))).tolist()\n",
    "\n",
    "# Seeds\n",
    "MASTER_SEED = 42\n",
    "N_SEEDS = N_MODEL_SIZES * N_DATASET_SIZES\n",
    "random.seed(MASTER_SEED)\n",
    "SEEDS = random.sample(range(10 * N_SEEDS), k=N_SEEDS)\n",
    "\n",
    "# Check\n",
    "print(f\"{MODEL_SIZES = }\")\n",
    "print(f\"{DATASET_FRACTIONS = }\")\n",
    "print(f\"{SEEDS = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, slots=True)\n",
    "class TrainingResult:\n",
    "    model_size: int\n",
    "    dataset_fraction: float\n",
    "    model: CNN\n",
    "    ds: Dataset\n",
    "    train_loss: float\n",
    "    test_loss: float\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "\n",
    "def acc_fn(\n",
    "    logits: t.Tensor, y: t.Tensor, *, as_pct: bool = True, pct_round_digits: int = 2\n",
    ") -> float:\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = (preds == y).to(dtype=t.float).mean().item()\n",
    "    if as_pct:\n",
    "        acc = round(100 * acc, pct_round_digits + 2)\n",
    "    return acc\n",
    "\n",
    "def train(seed: int, model_size: int, dataset_fraction: float) -> TrainingResult:\n",
    "    random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "    model = CNN(model_size)\n",
    "    ds = Dataset.load(dataset_fraction)\n",
    "    LR = 1e-3\n",
    "    optimizer = t.optim.AdamW(model.parameters(), lr=LR)\n",
    "    # scheduler = t.optim.lr_scheduler.ReduceLROnPlateau(optimizer, )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_logits = model(ds.train_x)\n",
    "    train_loss = loss_fn(train_logits, ds.train_y)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Measure\n",
    "    with t.no_grad():\n",
    "        # Training set \n",
    "        train_logits = model(ds.train_x)\n",
    "        train_loss = loss_fn(train_logits, ds.train_y).item()\n",
    "        train_acc = acc_fn(train_logits, ds.train_y)\n",
    "        \n",
    "        # Test set\n",
    "        test_logits = model(ds.test_x)\n",
    "        test_loss = loss_fn(test_logits, ds.test_y).item()\n",
    "        test_acc = acc_fn(test_logits, ds.test_y)\n",
    "    \n",
    "    return TrainingResult(\n",
    "        model_size,\n",
    "        dataset_fraction,\n",
    "        model,\n",
    "        ds,\n",
    "        train_loss,\n",
    "        test_loss,\n",
    "        train_acc,\n",
    "        test_acc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training accuracy: 8.8867%\n",
      "Initial test accuracy: 8.21%\n"
     ]
    }
   ],
   "source": [
    "model = CNN(MODEL_SIZES[0])\n",
    "random.seed(SEEDS[0])\n",
    "ds = Dataset.load(DATASET_FRACTIONS[0])\n",
    "train_logits = model(ds.train_x)\n",
    "test_logits = model(ds.test_x)\n",
    "\n",
    "print(f\"Initial training accuracy: {acc_fn(train_logits, ds.train_y)}%\")\n",
    "print(f\"Initial test accuracy: {acc_fn(test_logits, ds.test_y)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSize = int\n",
    "DatasetFraction = float\n",
    "Seed = int\n",
    "Param = tuple[Seed, ModelSize, DatasetFraction]\n",
    "PARAMS: list[Param] = [\n",
    "    (seed, model_size, dataset_fraction)\n",
    "    for seed, (model_size, dataset_fraction) in\n",
    "    zip(SEEDS, it.product(MODEL_SIZES, DATASET_FRACTIONS), strict=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [05:21<01:19,  3.99s/it]"
     ]
    }
   ],
   "source": [
    "results: dict[Param, TrainingResult] = {}\n",
    "\n",
    "for seed, model_size, dataset_fraction in tqdm(PARAMS):\n",
    "    tr = train(seed, model_size, dataset_fraction)\n",
    "    results[(seed, model_size, dataset_fraction)] = tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=90\n",
      "[0/90] model: 1, dataset: 0\n",
      "[1/90] model: 1, dataset: 1\n",
      "[2/90] model: 1, dataset: 2\n",
      "[3/90] model: 1, dataset: 3\n",
      "[4/90] model: 1, dataset: 4\n",
      "[5/90] model: 1, dataset: 5\n",
      "[6/90] model: 1, dataset: 6\n",
      "[7/90] model: 1, dataset: 7\n",
      "[8/90] model: 1, dataset: 8\n",
      "[9/90] model: 1, dataset: 9\n",
      "[10/90] model: 2, dataset: 0\n",
      "[11/90] model: 2, dataset: 1\n",
      "[12/90] model: 2, dataset: 2\n",
      "[13/90] model: 2, dataset: 3\n",
      "[14/90] model: 2, dataset: 4\n",
      "[15/90] model: 2, dataset: 5\n",
      "[16/90] model: 2, dataset: 6\n",
      "[17/90] model: 2, dataset: 7\n",
      "[18/90] model: 2, dataset: 8\n",
      "[19/90] model: 2, dataset: 9\n",
      "[20/90] model: 3, dataset: 0\n",
      "[21/90] model: 3, dataset: 1\n",
      "[22/90] model: 3, dataset: 2\n",
      "[23/90] model: 3, dataset: 3\n",
      "[24/90] model: 3, dataset: 4\n",
      "[25/90] model: 3, dataset: 5\n",
      "[26/90] model: 3, dataset: 6\n",
      "[27/90] model: 3, dataset: 7\n",
      "[28/90] model: 3, dataset: 8\n",
      "[29/90] model: 3, dataset: 9\n",
      "[30/90] model: 4, dataset: 0\n",
      "[31/90] model: 4, dataset: 1\n",
      "[32/90] model: 4, dataset: 2\n",
      "[33/90] model: 4, dataset: 3\n",
      "[34/90] model: 4, dataset: 4\n",
      "[35/90] model: 4, dataset: 5\n",
      "[36/90] model: 4, dataset: 6\n",
      "[37/90] model: 4, dataset: 7\n",
      "[38/90] model: 4, dataset: 8\n",
      "[39/90] model: 4, dataset: 9\n",
      "[40/90] model: 5, dataset: 0\n",
      "[41/90] model: 5, dataset: 1\n",
      "[42/90] model: 5, dataset: 2\n",
      "[43/90] model: 5, dataset: 3\n",
      "[44/90] model: 5, dataset: 4\n",
      "[45/90] model: 5, dataset: 5\n",
      "[46/90] model: 5, dataset: 6\n",
      "[47/90] model: 5, dataset: 7\n",
      "[48/90] model: 5, dataset: 8\n",
      "[49/90] model: 5, dataset: 9\n",
      "[50/90] model: 6, dataset: 0\n",
      "[51/90] model: 6, dataset: 1\n",
      "[52/90] model: 6, dataset: 2\n",
      "[53/90] model: 6, dataset: 3\n",
      "[54/90] model: 6, dataset: 4\n",
      "[55/90] model: 6, dataset: 5\n",
      "[56/90] model: 6, dataset: 6\n",
      "[57/90] model: 6, dataset: 7\n",
      "[58/90] model: 6, dataset: 8\n",
      "[59/90] model: 6, dataset: 9\n",
      "[60/90] model: 7, dataset: 0\n",
      "[61/90] model: 7, dataset: 1\n",
      "[62/90] model: 7, dataset: 2\n",
      "[63/90] model: 7, dataset: 3\n",
      "[64/90] model: 7, dataset: 4\n",
      "[65/90] model: 7, dataset: 5\n",
      "[66/90] model: 7, dataset: 6\n",
      "[67/90] model: 7, dataset: 7\n",
      "[68/90] model: 7, dataset: 8\n",
      "[69/90] model: 7, dataset: 9\n",
      "[70/90] model: 8, dataset: 0\n"
     ]
    }
   ],
   "source": [
    "MODEL_SIZES: list[int] = list(range(1, 10))\n",
    "DATASET_FRACTIONS: list[int] = list(range(0, 10))\n",
    "\n",
    "PARAMS = list(product(MODEL_SIZES, DATASET_FRACTIONS))\n",
    "N = len(PARAMS)\n",
    "print(f\"{N=}\")\n",
    "\n",
    "trs: list[TrainingResult] = []\n",
    "\n",
    "for param_i, (model_size, dataset_size) in enumerate(PARAMS):\n",
    "    print(f\"[{param_i}/{N}] model: {model_size}, dataset: {dataset_size}\")\n",
    "    cfg = Config(model_size, dataset_size)\n",
    "    tr = train(cfg)\n",
    "    trs.append(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_SIZES = [3, 6, 9, 12, 15, 18]\n",
      "DATASET_SIZES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "MODEL_SIZES: list[int] = np.arange(3, 20, step=3).tolist()\n",
    "DATASET_FRACTIONS: list[float] = np.linspace(0.1, 1, 10).round(1).tolist()\n",
    "print(f\"{MODEL_SIZES = }\\n{DATASET_FRACTIONS = }\")\n",
    "\n",
    "class Result(NamedTuple):\n",
    "    cfg: Config\n",
    "    cnn: CNN\n",
    "    train_loss: float\n",
    "    test_loss: float\n",
    "    train_acc: float\n",
    "    test_acc: float\n",
    "\n",
    "def acc_fn(logits: t.Tensor, y: t.Tensor) -> float:\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = (preds == y).to(t.float).mean().item()\n",
    "    return acc\n",
    "\n",
    "def train(model_size: int, dataset_size: float) -> Result:\n",
    "    # Setup\n",
    "    cfg = Config(model_size=model_size, dataset_size=dataset_size)\n",
    "    cnn = CNN(cfg)\n",
    "    ds = Dataset.make(cfg)    \n",
    "    optimizer = t.optim.AdamW(cnn.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Training\n",
    "    train_logits = cnn(ds.train_x)\n",
    "    train_loss = loss_fn(train_logits, ds.train_y)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    # Eval\n",
    "    with t.inference_mode():\n",
    "        train_logits = cnn(ds.train_x)\n",
    "        train_loss = loss_fn(train_logits, ds.train_y).item()\n",
    "        test_logits = cnn(ds.test_x)\n",
    "        test_loss = loss_fn(test_logits, ds.test_y).item()\n",
    "        train_acc = acc_fn(train_logits, ds.train_y)\n",
    "        test_acc = acc_fn(test_logits, ds.test_y)\n",
    "    return Result(\n",
    "        cfg=cfg,\n",
    "        cnn=cnn,\n",
    "        train_loss=train_loss,\n",
    "        test_loss=test_loss,\n",
    "        train_acc=train_acc,\n",
    "        test_acc=test_acc\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/60]: model_size=3, dataset_size=0.1\n",
      "[1/60]: model_size=3, dataset_size=0.2\n",
      "[2/60]: model_size=3, dataset_size=0.3\n",
      "[3/60]: model_size=3, dataset_size=0.4\n",
      "[4/60]: model_size=3, dataset_size=0.5\n",
      "[5/60]: model_size=3, dataset_size=0.6\n",
      "[6/60]: model_size=3, dataset_size=0.7\n",
      "[7/60]: model_size=3, dataset_size=0.8\n",
      "[8/60]: model_size=3, dataset_size=0.9\n",
      "[9/60]: model_size=3, dataset_size=1.0\n",
      "[10/60]: model_size=6, dataset_size=0.1\n",
      "[11/60]: model_size=6, dataset_size=0.2\n",
      "[12/60]: model_size=6, dataset_size=0.3\n",
      "[13/60]: model_size=6, dataset_size=0.4\n",
      "[14/60]: model_size=6, dataset_size=0.5\n",
      "[15/60]: model_size=6, dataset_size=0.6\n",
      "[16/60]: model_size=6, dataset_size=0.7\n",
      "[17/60]: model_size=6, dataset_size=0.8\n",
      "[18/60]: model_size=6, dataset_size=0.9\n",
      "[19/60]: model_size=6, dataset_size=1.0\n",
      "[20/60]: model_size=9, dataset_size=0.1\n",
      "[21/60]: model_size=9, dataset_size=0.2\n",
      "[22/60]: model_size=9, dataset_size=0.3\n",
      "[23/60]: model_size=9, dataset_size=0.4\n",
      "[24/60]: model_size=9, dataset_size=0.5\n",
      "[25/60]: model_size=9, dataset_size=0.6\n",
      "[26/60]: model_size=9, dataset_size=0.7\n",
      "[27/60]: model_size=9, dataset_size=0.8\n",
      "[28/60]: model_size=9, dataset_size=0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (model_size, dataset_size) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(PARAMS):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mN\u001b[39m}\u001b[39;00m\u001b[39m]: \u001b[39m\u001b[39m{\u001b[39;00mmodel_size\u001b[39m=}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mdataset_size\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     result \u001b[39m=\u001b[39m train(model_size, dataset_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(train_logits, ds\u001b[39m.\u001b[39mtrain_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/2.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Eval\u001b[39;00m\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PARAMS = list(product(MODEL_SIZES, DATASET_FRACTIONS))\n",
    "N = len(PARAMS)\n",
    "\n",
    "results: list[Result] = []\n",
    "for i, (model_size, dataset_size) in enumerate(PARAMS):\n",
    "    print(f\"[{i}/{N}]: {model_size=}, {dataset_size=}\")\n",
    "    result = train(model_size, dataset_size)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
