{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert any(\"deep_learning_curriculum\" in p for p in sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import functional as F, nn, optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from config import Config\n",
    "from model import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on reversing random tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDK what I'm doing wrong but I was unable to get it to more than ~35% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data: 100%|██████████| 128/128 [00:00<00:00, 52485.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape = torch.Size([4, 32, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model (and task) configuration\n",
    "cfg = Config(\n",
    "    d_model=128,\n",
    "    d_vocab=12,\n",
    "    n_layers=3,\n",
    "    n_heads=4,\n",
    "    n_ctx=12,\n",
    ")\n",
    "\n",
    "# Number of sequences of tokens  \n",
    "\n",
    "START_TOKEN = cfg.d_vocab - 1\n",
    "MID_TOKEN = cfg.d_vocab - 2\n",
    "MAX_VOCAB_TOKEN = cfg.d_vocab - 3\n",
    "SEQ_LEN = (cfg.n_ctx - 1) // 2\n",
    "MID_TOKEN_POS = SEQ_LEN + 1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_BATCHES = 4\n",
    "N = BATCH_SIZE * N_BATCHES\n",
    "# assert N % BATCH_SIZE == 0, f\"{N = }; {BATCH_SIZE = }\"\n",
    "\n",
    "data = t.zeros(N, cfg.n_ctx).to(dtype=t.int64)\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Generating data\"):\n",
    "    seq0 = random.sample(range(0, MAX_VOCAB_TOKEN + 1), k=SEQ_LEN)\n",
    "    seq1 = seq0[::-1]\n",
    "    data[i, :] = t.tensor([START_TOKEN, *seq0, MID_TOKEN, *seq1])\n",
    "    \n",
    "\n",
    "data = data.reshape(-1, BATCH_SIZE, cfg.n_ctx) # t.tensor(seqs).reshape(-1, BATCH_SIZE, cfg.n_ctx)\n",
    "\n",
    "print(f\"{data.shape = }\")\n",
    "\n",
    "assert data[0, 0, MID_TOKEN_POS] == MID_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits: t.Tensor, tokens: t.Tensor) -> t.Tensor:\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens)[..., 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "def acc_fn(logits: t.Tensor, tokens: t.Tensor) -> float:\n",
    "    logits = logits[:, MID_TOKEN_POS:-1]\n",
    "    tokens = tokens[:, MID_TOKEN_POS + 1:]\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = (tokens == preds).mean(dtype=t.float64).item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, slots=True)\n",
    "class TrainingHistory:\n",
    "    losses: list[float] = field(default_factory=list)\n",
    "    accuracies: list[float] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert len(self.losses) == len(self.accuracies)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.losses)\n",
    "\n",
    "    def append(self, loss: float, acc: float) -> None:\n",
    "        assert isinstance(loss, float)\n",
    "        assert isinstance(acc, float)\n",
    "        self.losses.append(loss)\n",
    "        self.accuracies.append(acc)\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    data: t.Tensor,\n",
    "    optimizer: Optimizer,\n",
    "    *,\n",
    "    n_epochs: int = 20,\n",
    "    log_each_epochs: int | None = 10,\n",
    "    scheduler: optim.lr_scheduler.LRScheduler | optim.lr_scheduler.ReduceLROnPlateau | None = None, # type: ignore\n",
    "    early_stopping_epochs: int | None = 3\n",
    ") -> TrainingHistory:\n",
    "    th = TrainingHistory()\n",
    "\n",
    "    for epoch_i in range(n_epochs):\n",
    "        \n",
    "        batch_losses: list[float] = []\n",
    "        batch_accs: list[float] = []\n",
    "        \n",
    "        for batch in data:\n",
    "            batch_logits = model(batch)\n",
    "            batch_loss = loss_fn(batch_logits, batch)\n",
    "            batch_acc = acc_fn(batch_logits, batch)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            batch_losses.append(batch_loss.item())\n",
    "            batch_accs.append(batch_acc)\n",
    "        \n",
    "        loss = np.mean(batch_losses).item()\n",
    "        acc = np.mean(batch_accs).item()\n",
    "        \n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(loss)\n",
    "        elif scheduler is not None:\n",
    "            scheduler.step()                \n",
    "    \n",
    "        th.append(loss=loss, acc=acc)\n",
    "        \n",
    "        if log_each_epochs is not None and epoch_i % log_each_epochs == 0:\n",
    "            print(f\"[{epoch_i}] {loss=:.3f}; {acc=:.2%}\")\n",
    "            \n",
    "        if early_stopping_epochs is not None and len(th.accuracies) >= early_stopping_epochs and np.prod(th.accuracies[-early_stopping_epochs:]) == 1:\n",
    "            print(f\"Leaving early after {epoch_i} epochs. Accuracy has stayed at 100% for the last {early_stopping_epochs} epochs.\")\n",
    "            break\n",
    "        \n",
    "    return th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "LR = 1e-3\n",
    "\n",
    "log_each_epochs = 10 # N_EPOCHS // 50\n",
    "\n",
    "model = Transformer(cfg)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda i: i / 1000 if i % 100 == 0 and i > 100 else 1)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch_i: max(i/1))\n",
    "# scheduler = None # optim.lr_scheduler.LambdaLR(optimizer, lambda i: max(i/100, 1), verbose=False)\n",
    "# optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10)\n",
    "# optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.2, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss=2.481; acc=9.38%\n",
      "[10] loss=2.129; acc=14.53%\n",
      "[20] loss=2.077; acc=16.56%\n",
      "[30] loss=2.376; acc=16.88%\n",
      "[40] loss=2.073; acc=16.56%\n",
      "[50] loss=2.059; acc=15.47%\n",
      "[60] loss=2.054; acc=17.03%\n",
      "[70] loss=2.048; acc=15.78%\n",
      "[80] loss=2.042; acc=15.94%\n",
      "[90] loss=2.037; acc=14.84%\n",
      "[100] loss=2.032; acc=15.47%\n",
      "[110] loss=2.027; acc=16.25%\n",
      "[120] loss=2.017; acc=15.94%\n",
      "[130] loss=2.005; acc=16.88%\n",
      "[140] loss=1.992; acc=15.94%\n",
      "[150] loss=1.964; acc=20.16%\n",
      "[160] loss=2.009; acc=16.09%\n",
      "[170] loss=1.995; acc=16.41%\n",
      "[180] loss=1.982; acc=17.50%\n",
      "[190] loss=1.952; acc=18.44%\n",
      "[200] loss=1.943; acc=20.62%\n",
      "[210] loss=1.924; acc=21.41%\n",
      "[220] loss=1.907; acc=24.06%\n",
      "[230] loss=1.947; acc=21.41%\n",
      "[240] loss=1.876; acc=22.97%\n",
      "[250] loss=1.853; acc=27.97%\n",
      "[260] loss=1.809; acc=30.31%\n",
      "[270] loss=1.773; acc=35.31%\n",
      "[280] loss=1.714; acc=39.53%\n",
      "[290] loss=1.679; acc=39.84%\n",
      "[300] loss=1.597; acc=44.06%\n",
      "[310] loss=1.520; acc=50.62%\n",
      "[320] loss=1.435; acc=56.25%\n",
      "[330] loss=1.316; acc=63.44%\n",
      "[340] loss=1.273; acc=70.16%\n",
      "[350] loss=1.116; acc=80.16%\n",
      "[360] loss=0.989; acc=87.66%\n",
      "[370] loss=0.953; acc=88.75%\n",
      "[380] loss=0.813; acc=97.19%\n",
      "[390] loss=0.747; acc=98.75%\n",
      "Leaving early after 397 epochs. Accuracy has stayed at 100% for the last 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "th = train(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    log_each_epochs=log_each_epochs,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().isoformat(\"T\", \"minutes\").replace(\":\", \"-\")\n",
    "filename = f\"model_reversal_{timestamp}.pt\"\n",
    "\n",
    "t.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tokens: t.Tensor, n_next_tokens: int = 1, *, verbose: bool = False) -> t.Tensor:\n",
    "    assert tokens.ndim == 2\n",
    "    assert n_next_tokens > 0\n",
    "    for i in tqdm(range(n_next_tokens), disable=not verbose):\n",
    "        logits = model(tokens)\n",
    "        preds = logits.argmax(-1)\n",
    "        next_tokens = preds[..., -1:]\n",
    "        # print(i, tokens.tolist(), next_tokens.tolist())\n",
    "        tokens = t.cat([tokens, next_tokens], dim=-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat = data.reshape(-1, data.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    seq = data_flat[i].reshape(1, -1)\n",
    "    seq_pre = seq[:, :MID_TOKEN_POS + 1]\n",
    "    seq_post = seq[:, MID_TOKEN_POS + 1:]\n",
    "    n_next_tokens = seq_post.size(-1)\n",
    "\n",
    "    generated = generate(seq_pre, n_next_tokens)\n",
    "    pred = generated[:, -n_next_tokens:]\n",
    "    logits = model(seq)\n",
    "    acc = acc_fn(logits, seq)\n",
    "    \n",
    "    if (seq_post != pred).any() and acc == 1:\n",
    "        print(f\"[{i}]\")\n",
    "        print(f\"seq: {seq.tolist()}\")\n",
    "        print(f\"post: {seq_post.tolist()}\")\n",
    "        print(f\"pred: {pred.tolist()}\")\n",
    "        print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: [[11, 5, 9, 1, 6, 0, 10, 0, 6, 1, 9, 5]]\n",
      "pre: [[11, 5, 9, 1, 6, 0, 10]]\n",
      "post: [[0, 6, 1, 9, 5]]\n",
      "pred: [[0, 6, 1, 9, 5]]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "seq = data_flat[i].reshape(1, -1)\n",
    "seq_pre = seq[:, :MID_TOKEN_POS + 1]\n",
    "seq_post = seq[:, MID_TOKEN_POS + 1:]\n",
    "n_next_tokens = seq_post.size(-1)\n",
    "\n",
    "generated = generate(seq_pre, n_next_tokens)\n",
    "pred = generated[:, -n_next_tokens:]\n",
    "\n",
    "print(\"seq:\", seq.tolist())\n",
    "print(\"pre:\", seq_pre.tolist())\n",
    "print(\"post:\", seq_post.tolist())\n",
    "print(\"pred:\", pred.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
