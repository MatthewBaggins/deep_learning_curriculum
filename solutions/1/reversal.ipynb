{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert any(\"deep_learning_curriculum\" in p for p in sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5000, 1000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from typing import cast, Self\n",
    "\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_last_dim(x: t.Tensor, max_dim: int) -> t.Tensor:\n",
    "    assert x.ndim > 0\n",
    "    assert x.size(-1) <= max_dim\n",
    "    if x.size(-1) == max_dim:\n",
    "        return x\n",
    "    padding = t.zeros(*x.shape[:-1], max_dim - x.size(-1))\n",
    "    return t.cat((x, padding), dim=-1)\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split text into words/tokens based on word boundaries (`\\\\b`).\"\"\"\n",
    "    return re.split(r\"\\b\", text)\n",
    "\n",
    "def to_one_hot(x: t.Tensor, max_last_dim: int | None = None) -> t.Tensor:\n",
    "    assert x.ndim == 2\n",
    "    x_one_hot = F.one_hot(x)\n",
    "    return pad_last_dim(x_one_hot, max_last_dim or x_one_hot.size(-1)).to(dtype=t.float32)\n",
    "\n",
    "def is_one_hot(x: t.Tensor) -> bool:\n",
    "    assert x.ndim >= 2\n",
    "    return cast(bool, ((x != 0).sum(-1) == 1).all().item())\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Config:\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_ctx: int\n",
    "    dropout: float = field(default=0.0, kw_only=True, repr=False)\n",
    "    epsilon: float = field(default=1e-6, kw_only=True, repr=False)\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        assert 0 <= self.dropout <= .9, f\"unreasonable dropout: {self.dropout}\"\n",
    "        \n",
    "    @property\n",
    "    def d_mlp(self) -> int:\n",
    "        return self.d_model * 4\n",
    "    \n",
    "    @property\n",
    "    def d_head(self) -> int:\n",
    "        return self.d_model // self.n_heads\n",
    "    \n",
    "    @classmethod\n",
    "    def dummy(cls, **kwargs) -> Self:\n",
    "        return cls(**(dict(d_model=20, n_ctx=5000, d_vocab=1000, n_layers=0, n_heads=1) | kwargs))\n",
    "    \n",
    "    def random_resid(self, n_batches: int = 16) -> t.Tensor:\n",
    "        return t.rand(n_batches, self.n_ctx, self.d_model)\n",
    "    \n",
    "    def random_tokens(self, n_batches: int = 16, n_ctx: int | None = None, max_token: int | None = None) -> t.Tensor:\n",
    "        n_ctx = n_ctx or self.n_ctx\n",
    "        max_token = max_token or self.d_vocab - 1\n",
    "        tokens = (max_token * t.rand(n_batches, n_ctx)).round().to(dtype=t.int64)\n",
    "        return tokens\n",
    "    \n",
    "    def random_tokens_one_hot(self, n_batches: int = 16, max_token: int | None = None) -> t.Tensor:\n",
    "        tokens = self.random_tokens(n_batches=n_batches, max_token=max_token)\n",
    "        tokens_one_hot = to_one_hot(tokens, self.d_vocab)\n",
    "        assert tokens_one_hot.shape[-2:] == (self.n_ctx, self.d_vocab)\n",
    "        assert is_one_hot(tokens_one_hot)\n",
    "        return tokens_one_hot\n",
    "    \n",
    "Config.dummy().random_tokens_one_hot().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED!\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty(cfg.n_ctx, cfg.d_model))\n",
    "        nn.init.normal_(self.W_pos)\n",
    "    \n",
    "    def forward(self, tokens: t.Tensor) -> t.Tensor:\n",
    "        pe = self.W_pos[:tokens.size(-1), :]\n",
    "        batch_pe = einops.repeat(pe, \"pos d_model -> batch pos d_model\", batch=tokens.size(0))\n",
    "        return batch_pe.clone()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        pe = cls(cfg)\n",
    "        x = cfg.random_tokens()\n",
    "        y = pe(x)\n",
    "        assert x.ndim == 2\n",
    "        assert y.ndim == 3\n",
    "        assert x.shape[:2] == y.shape[:2]\n",
    "        assert y.size(-1) == cfg.d_model\n",
    "        print(\"PASSED!\")\n",
    "    \n",
    "PosEmbed.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs.shape = torch.Size([16, 5000, 20])\n",
      "PASSED!\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty(cfg.d_vocab, cfg.d_model))\n",
    "        nn.init.normal_(self.W_E)\n",
    "        \n",
    "    def forward(self, tokens: t.Tensor) -> None:\n",
    "        assert tokens.ndim == 2\n",
    "        assert tokens.max() <= self.cfg.d_vocab - 1\n",
    "        assert tokens.size(1) <= self.cfg.n_ctx\n",
    "        \n",
    "        tokens_one_hot = to_one_hot(tokens, self.cfg.d_vocab)\n",
    "        # print(tokens_one_hot.shape, self.cfg.d_vocab)\n",
    "        return einsum(\n",
    "            \"batch pos d_vocab, d_vocab d_model -> batch pos d_model\",\n",
    "            tokens_one_hot,\n",
    "            self.W_E\n",
    "        ) * math.sqrt(self.cfg.d_model)\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        embed = cls(cfg)\n",
    "        tokens = cfg.random_tokens()\n",
    "        embs = embed(tokens)\n",
    "        assert embs.isnan().sum().item() == 0\n",
    "        # print(f\"{embs.shape = }\")\n",
    "        print(\"PASSED!\")\n",
    "        \n",
    "Embed.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED! (16, 5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty(cfg.d_model, cfg.d_vocab))\n",
    "        nn.init.normal_(self.W_U)\n",
    "        self.b_U = nn.Parameter(t.zeros(cfg. d_vocab))\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return einsum(\n",
    "            \"batch pos d_model, d_model d_vocab -> batch pos d_vocab\",\n",
    "            x,\n",
    "            self.W_U\n",
    "        ) + self.b_U\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        ue = cls(cfg)\n",
    "        x = cfg.random_resid()\n",
    "        y = ue(x)\n",
    "        print(f\"PASSED! {tuple(y.shape)}\")\n",
    "        # print(y.isnan().sum())\n",
    "        \n",
    "Unembed.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED! shape: (16, 5000, 20)\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Linear(cfg.d_model, cfg.d_mlp)\n",
    "        self.dropout = nn.Dropout(p=cfg.dropout)\n",
    "        self.W_out = nn.Linear(cfg.d_mlp, cfg.d_model)\n",
    "    \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return self.W_out(self.dropout(F.relu(self.W_in(x))))\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        mlp = cls(cfg)\n",
    "        x = cfg.random_resid()\n",
    "        y = mlp(x)\n",
    "        assert x.shape == y.shape\n",
    "        assert y.isnan().sum().item() == 0\n",
    "        print(f\"PASSED! shape: {tuple(x.shape)}\")\n",
    "\n",
    "MLP.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! shape: (16, 5000, 20)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    IGNORE: t.Tensor\n",
    "    \n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # Q\n",
    "        self.W_Q = nn.Parameter(t.empty(cfg.d_model, cfg.n_heads, cfg.d_head))\n",
    "        nn.init.normal_(self.W_Q)\n",
    "        self.b_Q = nn.Parameter(t.zeros(cfg.n_heads, cfg.d_head))\n",
    "        # K\n",
    "        self.W_K = nn.Parameter(t.empty(cfg.d_model, cfg.n_heads, cfg.d_head))\n",
    "        nn.init.normal_(self.W_K)\n",
    "        self.b_K = nn.Parameter(t.zeros(cfg.n_heads, cfg.d_head))\n",
    "        # V\n",
    "        self.W_V = nn.Parameter(t.empty(cfg.d_model, cfg.n_heads, cfg.d_head))\n",
    "        nn.init.normal_(self.W_V)\n",
    "        self.b_V = nn.Parameter(t.zeros(cfg.n_heads, cfg.d_head))\n",
    "        # O\n",
    "        self.W_O = nn.Parameter(t.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
    "        nn.init.normal_(self.W_O)\n",
    "        self.b_O = nn.Parameter(t.zeros(cfg.d_model))\n",
    "        # buffer\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e6))\n",
    "    \n",
    "    def apply_causal_mask(self, attn_scores: t.Tensor) -> t.Tensor:\n",
    "        assert attn_scores.ndim == 4\n",
    "        assert attn_scores.size(2) == attn_scores.size(3)\n",
    "        return attn_scores.where(\n",
    "            t.ones_like(attn_scores).triu().flip(-1) == 0,\n",
    "            self.IGNORE\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        assert x.ndim == 3 # batch pos d_model\n",
    "        assert x.size(2) == self.cfg.d_model\n",
    "        assert x.size(1) <= self.cfg.n_ctx\n",
    "        \n",
    "        q = einsum(\n",
    "            \"batch pos d_model, d_model n_heads d_head -> batch pos n_heads d_head\",\n",
    "            x, \n",
    "            self.W_Q\n",
    "        ) + self.b_Q\n",
    "        k = einsum(\n",
    "            \"batch pos d_model, d_model n_heads d_head -> batch pos n_heads d_head\",\n",
    "            x, \n",
    "            self.W_K\n",
    "        ) + self.b_K\n",
    "        v = einsum(\n",
    "            \"batch pos d_model, d_model n_heads d_head -> batch pos n_heads d_head\",\n",
    "            x, \n",
    "            self.W_V\n",
    "        ) + self.b_V\n",
    "        \n",
    "        attn_scores = einsum(\n",
    "            \"batch q_pos n_heads d_head, batch k_pos n_heads d_head -> batch n_heads q_pos k_pos\", \n",
    "            q,\n",
    "            k\n",
    "        ) / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "        \n",
    "        z = einsum(\n",
    "            \"batch n_heads q_pos k_pos, batch k_pos n_heads d_head -> batch q_pos n_heads d_head\",\n",
    "            attn_pattern,\n",
    "            v\n",
    "        )\n",
    "        \n",
    "        # print(f\"W_O: {tuple(self.W_O.shape)}\\nz: {tuple(z.shape)}\")\n",
    "        \n",
    "        o = einsum(\n",
    "            \"n_heads d_head d_model, batch pos n_heads d_head -> batch pos d_model\", \n",
    "            self.W_O,\n",
    "            z\n",
    "        ) + self.b_O\n",
    "        return o\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        x = cfg.random_resid()\n",
    "        attn = cls(cfg)\n",
    "        y = attn(x)\n",
    "        assert x.shape == y.shape\n",
    "        assert y.isnan().sum().item() == 0\n",
    "        print(f\"Passed! shape: {tuple(x.shape)}\")\n",
    "        \n",
    "SelfAttention.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED!\n"
     ]
    }
   ],
   "source": [
    "def normalize(x: t.Tensor, epsilon: float = 1e-6) -> t.Tensor:\n",
    "    return (x - x.mean(-1, keepdim=True)) / (x.std() + epsilon)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.scale = nn.Parameter(t.empty(cfg.d_model))\n",
    "        nn.init.normal_(self.scale)\n",
    "        self.translation = nn.Parameter(t.zeros(cfg.d_model))\n",
    "        \n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return normalize(x, self.cfg.epsilon) * self.scale + self.translation\n",
    "        \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        ln = cls(cfg)\n",
    "        x = cfg.random_resid()\n",
    "        y = ln(x)\n",
    "        assert x.shape == y.shape\n",
    "        assert y.isnan().sum().item() == 0\n",
    "        print(\"PASSED!\")\n",
    "        \n",
    "LayerNorm.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED!\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = SelfAttention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return x + self.mlp(self.ln2(self.attn(self.ln1(x))))\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy()\n",
    "        tb = cls(cfg)\n",
    "        x = cfg.random_resid()\n",
    "        y = tb(x)\n",
    "        assert x.shape == y.shape\n",
    "        assert y.isnan().sum().item() == 0\n",
    "        print(\"PASSED!\")\n",
    "\n",
    "TransformerBlock.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5000, 1000])\n",
      "torch.Size([16, 5000])\n",
      "Config(d_model=20, d_vocab=1000, n_layers=2, n_heads=1, n_ctx=5000)\n",
      "tensor([919, 993, 407,  ..., 522, 475, 244])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, cfg: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.embed = Embed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.unembed = Unembed(cfg)\n",
    "        \n",
    "    def forward(self, tokens: t.Tensor) -> t.Tensor:\n",
    "        assert tokens.ndim == 2\n",
    "        assert tokens.size(1) <= self.cfg.n_ctx\n",
    "        assert tokens.dtype == t.int64\n",
    "        assert tokens.max() <= self.cfg.d_vocab\n",
    "        \n",
    "        pos_embeddings = self.pos_embed(tokens)\n",
    "        embeddings = self.embed(tokens)\n",
    "        resid = pos_embeddings + embeddings\n",
    "        for block_i, block in enumerate(self.blocks):\n",
    "            resid = block(resid)\n",
    "        # print(\"check\", resid.isnan().sum().item())\n",
    "        logits = self.unembed(resid)\n",
    "        return logits\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls) -> None:\n",
    "        cfg = Config.dummy(n_layers=2)\n",
    "        transformer = cls(cfg)\n",
    "        tokens = cfg.random_tokens()\n",
    "        logits = transformer(tokens)\n",
    "        preds = logits.argmax(-1)\n",
    "        print(logits.shape)\n",
    "        print(preds.shape)\n",
    "        print(cfg)\n",
    "        nans = logits.isnan().sum().item()\n",
    "        assert nans == 0, f\"{nans = }\"\n",
    "        print(preds[0])\n",
    "        \n",
    "Transformer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on reversing random tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDK what I'm doing wrong but I was unable to get it to more than ~35% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 32, 10]), torch.Size([16, 32, 10]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = Config(\n",
    "    d_model=256,\n",
    "    d_vocab=10,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    n_ctx=10,\n",
    ")\n",
    "\n",
    "N = 1024\n",
    "TRAIN_FRAC = 0.5\n",
    "BATCH_SIZE = 32\n",
    "assert int(N * TRAIN_FRAC) % BATCH_SIZE == 0\n",
    "\n",
    "START_TOKEN = cfg.d_vocab - 1\n",
    "random_tokens = cfg.random_tokens(N, n_ctx=cfg.n_ctx // 2 - 1, max_token=START_TOKEN - 1)\n",
    "start_tokens = START_TOKEN * t.ones(N, 1)\n",
    "data = t.cat((start_tokens, random_tokens), dim=-1).to(dtype=t.int64)\n",
    "data = t.cat((data, data.flip(-1)), dim=-1)\n",
    "# random_tokens_reversed = random_tokens.flip(-1)\n",
    "# data = t.cat((start_tokens, random_tokens, random_tokens_reversed), dim=-1).to(dtype=t.int64)\n",
    "split_ind = int(TRAIN_FRAC * N)\n",
    "train_data_flat = data[:split_ind]\n",
    "train_data = train_data_flat.reshape(-1, BATCH_SIZE, cfg.n_ctx)\n",
    "test_data_flat = data[split_ind:]\n",
    "test_data = test_data_flat.reshape(-1, BATCH_SIZE, cfg.n_ctx)\n",
    "\n",
    "train_data.shape,test_data.shape\n",
    "\n",
    "# assert train_data.size(1) == cfg.n_ctx # - 1\n",
    "# assert train_data[:, 1:-1].max() == START_TOKEN - 1\n",
    "# assert train_data[:, 0].max() == train_data[:, 0].min() == START_TOKEN == train_data[:, -1].max() == train_data[:, -1].min()\n",
    "# assert train_data.dtype == t.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(422.6483, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(logits: t.Tensor, tokens: t.Tensor) -> t.Tensor:\n",
    "    # assert logits.shape[:2] == tokens.shape\n",
    "    # assert logits.size(1) % 2 == 1\n",
    "    # n_ctx = logits.size(1) // 2\n",
    "    # logits = logits[:, n_ctx:-1]\n",
    "    # tokens = tokens[:, n_ctx+1:].unsqueeze(-1)\n",
    "    # assert logits.ndim == tokens.ndim == 3\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens)[..., 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "def acc_fn(logits: t.Tensor, tokens: t.Tensor) -> float:\n",
    "    n_ctx = logits.size(1) // 2\n",
    "    logits = logits[:, n_ctx:-1]\n",
    "    preds = logits.argmax(-1)\n",
    "    tokens = tokens[:, n_ctx+1:]\n",
    "    acc = (preds == tokens).mean(dtype=t.float).item()\n",
    "    return acc\n",
    "    \n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Transformer(cfg)\n",
    "train_logits = model(train_data_flat)\n",
    "assert train_logits.isnan().sum().item() == 0\n",
    "loss = loss_fn(train_logits, train_data_flat)\n",
    "print(f\"{loss = }\")\n",
    "\n",
    "\n",
    "# loss = loss_fn(train_preds, train_data.to(dtype=t.float))\n",
    "\n",
    "\n",
    "# acc = acc_fn(train_logits, train_data)\n",
    "# print(f\"{loss = }; {acc = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] \n",
      "    loss: train=285.656; test=284.506;\n",
      "    acc:  train=11.72%; test=10.06%\n",
      "[20] \n",
      "    loss: train=80.662; test=88.524;\n",
      "    acc:  train=26.42%; test=22.31%\n",
      "[40] \n",
      "    loss: train=39.626; test=44.791;\n",
      "    acc:  train=33.74%; test=30.22%\n",
      "[60] \n",
      "    loss: train=32.910; test=36.502;\n",
      "    acc:  train=31.74%; test=29.83%\n",
      "[80] \n",
      "    loss: train=36.694; test=39.027;\n",
      "    acc:  train=29.49%; test=27.54%\n",
      "[100] \n",
      "    loss: train=38.767; test=40.889;\n",
      "    acc:  train=31.64%; test=30.32%\n",
      "[120] \n",
      "    loss: train=46.558; test=49.009;\n",
      "    acc:  train=28.42%; test=27.64%\n",
      "[140] \n",
      "    loss: train=52.575; test=53.422;\n",
      "    acc:  train=30.32%; test=29.20%\n",
      "[160] \n",
      "    loss: train=52.438; test=54.450;\n",
      "    acc:  train=30.62%; test=28.91%\n",
      "[180] \n",
      "    loss: train=64.006; test=65.967;\n",
      "    acc:  train=29.98%; test=29.74%\n",
      "[200] \n",
      "    loss: train=60.328; test=62.368;\n",
      "    acc:  train=28.08%; test=29.05%\n",
      "[220] \n",
      "    loss: train=58.725; test=59.570;\n",
      "    acc:  train=33.79%; test=32.91%\n",
      "[240] \n",
      "    loss: train=67.123; test=66.127;\n",
      "    acc:  train=33.79%; test=34.67%\n",
      "[260] \n",
      "    loss: train=64.394; test=64.603;\n",
      "    acc:  train=36.04%; test=34.67%\n",
      "[280] \n",
      "    loss: train=58.358; test=60.183;\n",
      "    acc:  train=29.83%; test=27.00%\n",
      "[300] \n",
      "    loss: train=61.502; test=63.296;\n",
      "    acc:  train=29.69%; test=28.08%\n",
      "[320] \n",
      "    loss: train=57.716; test=59.218;\n",
      "    acc:  train=32.52%; test=31.01%\n",
      "[340] \n",
      "    loss: train=67.011; test=67.107;\n",
      "    acc:  train=31.01%; test=31.93%\n",
      "[360] \n",
      "    loss: train=59.143; test=58.688;\n",
      "    acc:  train=34.28%; test=35.16%\n",
      "[380] \n",
      "    loss: train=62.599; test=62.793;\n",
      "    acc:  train=33.01%; test=32.96%\n",
      "[400] \n",
      "    loss: train=52.045; test=52.771;\n",
      "    acc:  train=33.40%; test=32.96%\n",
      "[420] \n",
      "    loss: train=51.309; test=51.945;\n",
      "    acc:  train=33.25%; test=30.86%\n",
      "[440] \n",
      "    loss: train=54.470; test=53.197;\n",
      "    acc:  train=34.81%; test=34.42%\n",
      "[460] \n",
      "    loss: train=54.379; test=53.554;\n",
      "    acc:  train=35.45%; test=32.86%\n",
      "[480] \n",
      "    loss: train=52.871; test=52.627;\n",
      "    acc:  train=33.79%; test=34.28%\n",
      "[500] \n",
      "    loss: train=47.883; test=46.507;\n",
      "    acc:  train=31.40%; test=31.01%\n",
      "[520] \n",
      "    loss: train=50.800; test=50.394;\n",
      "    acc:  train=30.76%; test=30.86%\n",
      "[540] \n",
      "    loss: train=41.137; test=41.435;\n",
      "    acc:  train=33.11%; test=33.69%\n",
      "[560] \n",
      "    loss: train=41.068; test=42.129;\n",
      "    acc:  train=32.91%; test=33.01%\n",
      "[580] \n",
      "    loss: train=40.957; test=43.376;\n",
      "    acc:  train=34.57%; test=33.64%\n",
      "[600] \n",
      "    loss: train=37.386; test=39.141;\n",
      "    acc:  train=33.79%; test=33.06%\n",
      "[620] \n",
      "    loss: train=37.964; test=39.850;\n",
      "    acc:  train=34.72%; test=33.98%\n",
      "[640] \n",
      "    loss: train=38.237; test=39.732;\n",
      "    acc:  train=33.98%; test=33.74%\n",
      "[660] \n",
      "    loss: train=35.514; test=36.999;\n",
      "    acc:  train=33.98%; test=33.89%\n",
      "[680] \n",
      "    loss: train=31.151; test=33.095;\n",
      "    acc:  train=34.23%; test=33.84%\n",
      "[700] \n",
      "    loss: train=30.422; test=31.746;\n",
      "    acc:  train=34.08%; test=34.72%\n",
      "[720] \n",
      "    loss: train=28.753; test=29.213;\n",
      "    acc:  train=32.96%; test=33.69%\n",
      "[740] \n",
      "    loss: train=25.224; test=25.078;\n",
      "    acc:  train=33.94%; test=33.54%\n",
      "[760] \n",
      "    loss: train=23.629; test=23.517;\n",
      "    acc:  train=34.28%; test=33.89%\n",
      "[780] \n",
      "    loss: train=21.304; test=21.728;\n",
      "    acc:  train=31.93%; test=32.76%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "LR = 1e-4\n",
    "# BETAS = (.9, .99)\n",
    "\n",
    "log_each_epochs = N_EPOCHS // 50\n",
    "\n",
    "model = Transformer(cfg)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "N_MILESTONES = 4\n",
    "milestones = np.linspace(N_EPOCHS // N_MILESTONES, N_EPOCHS, N_MILESTONES)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.1, patience=10)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch_i in range(N_EPOCHS):\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    epoch_train_accuracies = []\n",
    "    epoch_test_accuracies = []\n",
    "    for train_batch, test_batch in zip(train_data, test_data):\n",
    "        # Forward\n",
    "        train_logits = model(train_batch)\n",
    "        test_logits = model(test_batch)\n",
    "        # Loss\n",
    "        train_loss = loss_fn(train_logits, train_batch)\n",
    "        test_loss = loss_fn(test_logits, test_batch)\n",
    "        # Backward and update\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Accuracy\n",
    "        train_acc = acc_fn(train_logits, train_batch)\n",
    "        test_acc = acc_fn(test_logits, test_batch)\n",
    "        # Append\n",
    "        epoch_train_losses.append(train_loss.item())\n",
    "        epoch_test_losses.append(test_loss.item())\n",
    "        epoch_train_accuracies.append(train_acc)\n",
    "        epoch_test_accuracies.append(test_acc)\n",
    "        \n",
    "    \n",
    "    # Measure    \n",
    "    epoch_train_loss = t.tensor(epoch_train_losses).mean().item()\n",
    "    epoch_test_loss = t.tensor(epoch_test_losses).mean().item()\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "    epoch_train_acc = t.tensor(epoch_train_accuracies).mean().item()\n",
    "    epoch_test_acc = t.tensor(epoch_test_accuracies).mean().item()\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "    test_accuracies.append(epoch_test_acc)\n",
    "    \n",
    "    if log_each_epochs and epoch_i % log_each_epochs == 0:\n",
    "        print(f\"[{epoch_i}] \\n    loss: train={epoch_train_loss:.3f}; test={epoch_test_loss:.3f};\\n    acc:  train={epoch_train_acc:.2%}; test={epoch_test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 8, 1, 7, 3, 3, 7, 1, 8, 9]])\n",
      "tensor([[5, 0, 0, 3, 5, 5, 3, 5, 9, 2]])\n"
     ]
    }
   ],
   "source": [
    "batch = train_data[0][:1]\n",
    "logits = model(batch)\n",
    "preds = logits.argmax(-1)\n",
    "print(batch)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 11, 15,  5, 11, 29, 10, 19, 19, 10, 29, 11,  5, 15, 11, 11])\n",
      "tensor([26, 11,  7, 11,  9, 20, 11, 10, 19, 20, 11, 11,  8,  7, 11,  9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = model(train_data)\n",
    "preds = logits.argmax(-1)\n",
    "print(train_data[0])\n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26, 16, 28, 18,  7, 12, 14, 11, 11, 14, 12,  7, 18, 28, 16, 26])\n",
      "tensor([26,  6, 18, 26, 14,  5, 28, 12, 11,  7,  5, 14, 28, 18, 12, 22])\n"
     ]
    }
   ],
   "source": [
    "logits = model(test_data)\n",
    "preds = logits.argmax(-1)\n",
    "print(test_data[0])\n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 11, 15,  5, 11, 29, 10, 19, 19, 10, 29, 11,  5, 15, 11, 11]])\n",
      "tensor([[11, 11, 15,  5, 11, 29, 10, 19, 19, 10, 23, 15,  7,  4,  2, 25]])\n"
     ]
    }
   ],
   "source": [
    "def generate(model: Transformer, tokens: t.Tensor, max_new_tokens: int | None = None) -> t.Tensor:\n",
    "    assert tokens.ndim == 2\n",
    "    assert tokens.size(1) < model.cfg.n_ctx\n",
    "    max_new_tokens = max_new_tokens or model.cfg.n_ctx - tokens.size(-1)\n",
    "    new_tokens = tokens.detach().clone()\n",
    "    for i in range(max_new_tokens):\n",
    "        logits = model(new_tokens)\n",
    "        preds = logits.argmax(-1)\n",
    "        assert preds.ndim == 2\n",
    "        final_preds = preds[:, -1].unsqueeze(-1)\n",
    "        new_tokens = t.cat((new_tokens, final_preds), dim=-1)\n",
    "    return new_tokens\n",
    "\n",
    "x = train_data[:1]\n",
    "tokens = x[:, :model.cfg.n_ctx // 2 + 2]\n",
    "y = generate(model, tokens)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on Shakespeare's works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare...\n",
      "Shakespeare text: 5392638 characters, 1991703 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "shakespeare_path = pathlib.Path(\"../data/shakespeare.txt\")\n",
    "\n",
    "if shakespeare_path.exists():\n",
    "    print(\"Loading Shakespeare...\")\n",
    "    with open(shakespeare_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    print(\"Fetching Shakespeare..\")\n",
    "    url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "    text = urlopen(url).read().decode(\"utf-8\")\n",
    "    with open(shakespeare_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "\n",
    "print(f\"Shakespeare text: {len(text)} characters, {len(tokens)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', 'The', ' ', 'Project', ' ', 'Gutenberg', ' ', 'eBook', ' ', 'of', ' ', 'The', ' ', 'Complete', ' ', 'Works', ' ', 'of', ' ', 'William']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Iterable, TypeVar\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Tokenizer:\n",
    "    d_vocab: int\n",
    "    tok2int: dict[str, int]\n",
    "    int2tok :dict[int, str]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_tokens(text: str) -> list[str]:\n",
    "        return re.split(r\"\\b\", text)\n",
    "    \n",
    "    @classmethod\n",
    "    def make(cls, text: str) -> Tokenizer:\n",
    "        tokens = cls.split_into_tokens(text)\n",
    "        token_counts = Counter(tokens)\n",
    "        d_vocab = len(token_counts) + 1 # plus BOS/EOS\n",
    "        tok2int = {tok: i for i, (tok, _) in enumerate(sorted(token_counts.items(), key=lambda x: x[1], reverse=True))}\n",
    "        int2tok = {i: tok for tok, i in tok2int.items()}\n",
    "        assert len(tok2int) == d_vocab - 1\n",
    "        return cls(d_vocab, tok2int, int2tok)\n",
    "    \n",
    "    @property\n",
    "    def eos(self) -> int:\n",
    "        return self.d_vocab - 1\n",
    "    @property\n",
    "    def token_set(self) -> set[str]:\n",
    "        return set(self.tok2int)\n",
    "    \n",
    "    def tokenize(self, text: str) -> tuple[list[str], list[int]]:\n",
    "        tokens = self.split_into_tokens(text)\n",
    "        assert set(tokens) <= self.token_set\n",
    "        token_ids = [self.tok2int[tok] for tok in tokens]\n",
    "        return tokens, token_ids\n",
    "    \n",
    "    def decode(self, inds: Iterable[int]) -> list[str]:\n",
    "        assert all(i < self.d_vocab for i in inds)\n",
    "        return [self.int2tok[i] for i in inds]\n",
    "        \n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "def split_into_pieces(xs: list[T], n_pieces: int, piece_length: int) -> list[list[T]]:\n",
    "    assert n_pieces * piece_length < len(xs)\n",
    "    # max_start_ind = len(xs) - piece_length\n",
    "    # start_inds = [random.randint(0, max_start_ind) for _ in range(n_pieces)]\n",
    "    pieces = [\n",
    "        xs[i * piece_length : (i + 1) * piece_length] \n",
    "        for i in range(n_pieces)\n",
    "    ]\n",
    "    assert len(pieces) == n_pieces\n",
    "    assert all(len(p) == piece_length for p in pieces)\n",
    "    return pieces\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer.make(text)\n",
    "tokens, token_ids = tokenizer.tokenize(text)\n",
    "\n",
    "cfg = Config(\n",
    "    d_model=128,\n",
    "    d_vocab=tokenizer.d_vocab,\n",
    "    n_layers=4,\n",
    "    n_heads=8,\n",
    "    n_ctx=256,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "pieces = split_into_pieces(token_ids, n_pieces=256, piece_length=cfg.n_ctx)\n",
    "token_tensor = t.tensor(pieces)#.unsqueeze(0)\n",
    "\n",
    "print(tokenizer.decode(pieces[0])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits: t.Tensor, tokens: t.Tensor) -> t.Tensor:\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens).squeeze(-1)\n",
    "    return -correct_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 256])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_BATCHES = 8\n",
    "assert token_tensor.size(0) % N_BATCHES == 0\n",
    "batches = token_tensor.reshape(N_BATCHES, -1, token_tensor.size(-1))\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: loss = 497.613\n",
      "[1]: loss = 475.608\n",
      "[2]: loss = 456.961\n",
      "[3]: loss = 438.606\n",
      "[4]: loss = 417.623\n",
      "[5]: loss = 394.853\n",
      "[6]: loss = 370.869\n",
      "[7]: loss = 356.367\n",
      "[8]: loss = 350.797\n",
      "[9]: loss = 343.557\n",
      "[10]: loss = 333.555\n",
      "[11]: loss = 320.975\n",
      "[12]: loss = 308.607\n",
      "[13]: loss = 300.223\n",
      "[14]: loss = 294.453\n",
      "[15]: loss = 289.125\n",
      "[16]: loss = 283.711\n",
      "[17]: loss = 278.533\n",
      "[18]: loss = 274.038\n",
      "[19]: loss = 269.739\n",
      "[20]: loss = 265.175\n",
      "[21]: loss = 261.107\n",
      "[22]: loss = 257.682\n",
      "[23]: loss = 253.756\n",
      "[24]: loss = 248.863\n",
      "[25]: loss = 243.134\n",
      "[26]: loss = 237.000\n",
      "[27]: loss = 231.162\n",
      "[28]: loss = 226.516\n",
      "[29]: loss = 223.017\n",
      "[30]: loss = 220.411\n",
      "[31]: loss = 218.557\n",
      "[32]: loss = 216.727\n",
      "[33]: loss = 213.388\n",
      "[34]: loss = 207.911\n",
      "[35]: loss = 201.511\n",
      "[36]: loss = 195.534\n",
      "[37]: loss = 190.740\n",
      "[38]: loss = 187.545\n",
      "[39]: loss = 185.531\n",
      "[40]: loss = 183.982\n",
      "[41]: loss = 182.198\n",
      "[42]: loss = 179.854\n",
      "[43]: loss = 177.266\n",
      "[44]: loss = 174.394\n",
      "[45]: loss = 171.092\n",
      "[46]: loss = 167.486\n",
      "[47]: loss = 163.795\n",
      "[48]: loss = 160.272\n",
      "[49]: loss = 156.875\n",
      "[50]: loss = 153.682\n",
      "[51]: loss = 150.967\n",
      "[52]: loss = 148.832\n",
      "[53]: loss = 147.037\n",
      "[54]: loss = 145.374\n",
      "[55]: loss = 143.761\n",
      "[56]: loss = 142.026\n",
      "[57]: loss = 140.126\n",
      "[58]: loss = 138.156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Backward and update\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Append\u001b[39;00m\n",
      "File \u001b[0;32m~/code/tokensorting/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/code/tokensorting/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(cfg)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.1, patience=4)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "for epoch_i in range(N_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    for batch_i, batch in enumerate(batches):\n",
    "        # Forward\n",
    "        logits = model(batch)\n",
    "        # Loss\n",
    "        loss = loss_fn(logits, batch)\n",
    "        # Backward and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Append\n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    # Measure\n",
    "    epoch_loss = t.tensor(epoch_losses).mean().item()\n",
    "    loss_history.append(epoch_loss)\n",
    "    \n",
    "    print(f\"[{epoch_i}]: loss = {epoch_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "dt_str = datetime.now().isoformat().replace(\":\", \"-\").split(\".\")[0]\n",
    "model_filepath = f\"../models/model-1-{dt_str}.pkl\"\n",
    "with open(model_filepath, \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- add BOS token\n",
    "- retrain model with max number of splits\n",
    "- generate shakespeare or sth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
