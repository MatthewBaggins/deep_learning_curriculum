{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert any(\"deep_learning_curriculum\" in p for p in sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch as t\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from config import Config\n",
    "from model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH = PosixPath('/home/matthewbaggins/code/deep_learning_curriculum')\n"
     ]
    }
   ],
   "source": [
    "PATH = pathlib.Path(os.getcwd())\n",
    "while not str(PATH).endswith(\"_curriculum\"):\n",
    "    PATH = PATH.parent\n",
    "print(f\"{PATH = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on Shakespeare's works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = PATH / \"data\"\n",
    "\n",
    "def load_corpus_text() -> str:\n",
    "    if not data_path.exists():\n",
    "        data_path.mkdir()\n",
    "\n",
    "    shakespeare_path = data_path / \"shakespeare.txt\"\n",
    "\n",
    "    if shakespeare_path.exists():\n",
    "        print(\"Loading Shakespeare...\")\n",
    "        with open(shakespeare_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        print(\"Fetching Shakespeare..\")\n",
    "        url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "        text = urlopen(url).read().decode(\"utf-8\")\n",
    "        with open(shakespeare_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return re.split(r\"\\b\", text)\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Corpus:\n",
    "    text: str\n",
    "    tokens_str: list[str]\n",
    "    tokens_int: list[int]\n",
    "    tok_int2str: dict[int, str]\n",
    "    tok_str2int: dict[str, int]\n",
    "    token_counts: dict[str, int]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens_str)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls) -> Corpus:\n",
    "        text = load_corpus_text()\n",
    "        tokens_str = tokenize(text)\n",
    "        token_counts = Counter(tokens_str)\n",
    "        tok_int2str: dict[int, str] = {}\n",
    "        tok_str2int: dict[str, int] = {}\n",
    "        for i, (tok_str, tok_count) in enumerate(sorted(token_counts.items(), key=lambda x: x[1], reverse=True)):\n",
    "            tok_int2str[i] = tok_str\n",
    "            tok_str2int[tok_str] = i\n",
    "        tokens_int = [tok_str2int[tok_str] for tok_str in tokens_str]\n",
    "        corpus = cls(\n",
    "            text=text,\n",
    "            tokens_str=tokens_str,\n",
    "            tokens_int=tokens_int,\n",
    "            tok_int2str=tok_int2str,\n",
    "            tok_str2int=tok_str2int,\n",
    "            token_counts=dict(token_counts)\n",
    "        )\n",
    "        print(f\"Shakespeare text: {len(text)} characters, {len(tokens_str)} tokens\")\n",
    "        return corpus\n",
    "    \n",
    "    def get_corpus_subsequences(self, n_subsequences: int = 32) -> list[list[int]]:\n",
    "        n_tokens = len(self)\n",
    "        subseq_len = n_tokens // n_subsequences\n",
    "        subseqs = [self.tokens_int[i * subseq_len : (i + 1)* subseq_len] for i in range(n_subsequences)]\n",
    "        # seps = [i * (n // n_subsequences) for i in range(n_subsequences)]\n",
    "        # subseqs = [self.tokens_int[sep0:sep1] for sep0, sep1 in zip(seps, seps[1:])]\n",
    "        return subseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare...\n",
      "Shakespeare text: 5392638 characters, 1991703 tokens\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "subseqs = corpus.get_corpus_subsequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make model\n",
    "- preprocess each subseq such that it starts with EOS token\n",
    "- loss_fn, acc_fn, train (mostly copy pasting)\n",
    "- do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- add BOS token\n",
    "- retrain model with max number of splits\n",
    "- generate shakespeare or sth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
