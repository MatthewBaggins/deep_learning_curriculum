{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert any(\"deep_learning_curriculum\" in p for p in sys.path), sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import itertools as it\n",
    "import random\n",
    "from frozendict import frozendict\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import Config\n",
    "from model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH = PosixPath('/home/matthewbaggins/code/deep_learning_curriculum')\n"
     ]
    }
   ],
   "source": [
    "PATH = pathlib.Path(os.getcwd())\n",
    "while not str(PATH).endswith(\"_curriculum\"):\n",
    "    PATH = PATH.parent\n",
    "print(f\"{PATH = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on Shakespeare's works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = PATH / \"data\"\n",
    "\n",
    "def load_corpus_text() -> str:\n",
    "    if not data_path.exists():\n",
    "        data_path.mkdir()\n",
    "\n",
    "    shakespeare_path = data_path / \"shakespeare.txt\"\n",
    "\n",
    "    if shakespeare_path.exists():\n",
    "        print(\"Loading Shakespeare...\")\n",
    "        with open(shakespeare_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        print(\"Fetching Shakespeare..\")\n",
    "        url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "        text = urlopen(url).read().decode(\"utf-8\")\n",
    "        with open(shakespeare_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return re.split(r\"\\b\", text)\n",
    "\n",
    "EOS_TOKEN_STR = \"<EOS>\"\n",
    "EOS_TOKEN_INT = 0\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Corpus:\n",
    "    text: str\n",
    "    tokens_str: list[str]\n",
    "    tokens_int: list[int]\n",
    "    tok_int2str: frozendict[int, str]\n",
    "    tok_str2int: frozendict[str, int]\n",
    "    token_counts: frozendict[str, int]\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls) -> Corpus:\n",
    "        text = load_corpus_text()\n",
    "        tokens_str = tokenize(text)\n",
    "        token_counts = Counter(tokens_str)\n",
    "        tok_int2str: dict[int, str] = {EOS_TOKEN_INT: EOS_TOKEN_STR}\n",
    "        tok_str2int: dict[str, int] = {EOS_TOKEN_STR: EOS_TOKEN_INT}\n",
    "        for tok_int, (tok_str, _tok_count) in enumerate(\n",
    "            sorted(token_counts.items(), key=lambda x: x[1], reverse=True),\n",
    "            start=1\n",
    "        ):\n",
    "            assert tok_int not in tok_int2str\n",
    "            assert tok_str not in tok_str2int\n",
    "            tok_int2str[tok_int] = tok_str\n",
    "            tok_str2int[tok_str] = tok_int\n",
    "        tokens_int = [tok_str2int[tok_str] for tok_str in tokens_str]\n",
    "        corpus = cls(\n",
    "            text=text,\n",
    "            tokens_str=tokens_str,\n",
    "            tokens_int=tokens_int,\n",
    "            tok_int2str=frozendict(tok_int2str),\n",
    "            tok_str2int=frozendict(tok_str2int),\n",
    "            token_counts=frozendict(token_counts)\n",
    "        )\n",
    "        print(f\"Shakespeare text: {len(text)} characters, {len(tokens_str)} tokens\")\n",
    "        return corpus\n",
    "    \n",
    "    def get_corpus_subsequences(self, subseq_len: int, *, pad_with_eos: bool = True) -> t.Tensor: # [batch pos]\n",
    "        n_subseqs = len(self) // subseq_len\n",
    "        if pad_with_eos:\n",
    "            subseq_len -= 1\n",
    "        subseqs = t.tensor(\n",
    "            [\n",
    "                self.tokens_int[i * subseq_len : (i + 1) * subseq_len]\n",
    "                for i in range(n_subseqs)\n",
    "            ],\n",
    "            dtype=t.int64\n",
    "        )\n",
    "        if pad_with_eos:\n",
    "            subseqs = t.cat(\n",
    "                [\n",
    "                    t.tensor(list(it.repeat(EOS_TOKEN_INT, n_subseqs))).reshape(n_subseqs, 1),\n",
    "                    subseqs\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        return subseqs\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens_str)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare...\n",
      "Shakespeare text: 5392638 characters, 1991703 tokens\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits: t.Tensor, tokens: t.Tensor) -> t.Tensor:\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens)[..., 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "def acc_fn(logits: t.Tensor, tokens: t.Tensor) -> float:\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:]\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = (tokens == preds).mean(dtype=t.float64).item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, slots=True)\n",
    "class TrainingHistory:\n",
    "    losses: list[list[float]] = field(default_factory=list)\n",
    "    accuracies: list[list[float]] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert len(self.losses) == len(self.accuracies)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.losses)\n",
    "\n",
    "def stop_early(accuracies: list[float], early_stopping_epochs: int | None, min_acc: float = 1.0) -> bool:\n",
    "    if early_stopping_epochs is None:\n",
    "        return False\n",
    "    return all(acc >= min_acc for acc in accuracies)\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    batches: list[t.Tensor],\n",
    "    optimizer: optim.Optimizer,\n",
    "    *,\n",
    "    n_epochs: int = 20,\n",
    "    scheduler: optim.lr_scheduler.LRScheduler | optim.lr_scheduler.ReduceLROnPlateau | None = None, # type: ignore\n",
    ") -> TrainingHistory:\n",
    "    th = TrainingHistory()\n",
    "    n_batches = len(batches)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch_i} / {n_epochs}\")\n",
    "        \n",
    "        batch_losses: list[float] = []\n",
    "        batch_accs: list[float] = []\n",
    "        th.losses.append(batch_losses)\n",
    "        th.accuracies.append(batch_accs)\n",
    "        \n",
    "        for batch_i, batch in enumerate(batches, 1):\n",
    "            optimizer.zero_grad()\n",
    "            batch_logits = model(batch)\n",
    "            batch_loss = loss_fn(batch_logits, batch)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            with t.no_grad():\n",
    "                batch_acc = acc_fn(batch_logits, batch)\n",
    "            batch_losses.append(batch_loss.item())\n",
    "            batch_accs.append(batch_acc)\n",
    "            \n",
    "            msg = f\"\\t Batch {batch_i} / {n_batches}: loss={batch_losses[-1]:.3f}, acc={batch_acc:.3%}\"\n",
    "            \n",
    "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(batch_loss)\n",
    "                if hasattr(scheduler, \"_last_lr\"):\n",
    "                    last_lr: float = getattr(scheduler, \"_last_lr\")[-1]\n",
    "                    msg += f\", {last_lr=}\"\n",
    "            elif scheduler is not None:\n",
    "                scheduler.step()\n",
    "                last_lr: float = scheduler.get_last_lr()[-1]\n",
    "                msg += f\", {last_lr=}\"\n",
    "            \n",
    "            print(msg)\n",
    "                \n",
    "    return th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    d_model=128,\n",
    "    d_vocab=corpus.vocab_size + 1,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    n_ctx=128,\n",
    ")\n",
    "subseqs = corpus.get_corpus_subsequences(subseq_len=cfg.n_ctx - 1)\n",
    "BATCH_SIZE = 32\n",
    "N_BATCHES = len(subseqs) // BATCH_SIZE\n",
    "batches: list[t.Tensor] = [subseqs[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] for i in range(N_BATCHES)]\n",
    "random.shuffle(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(cfg)\n",
    "\n",
    "LR = 1e-3\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "N_EPOCHS = 2\n",
    "LOG_EACH_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2\n",
      "\t Batch 1 / 490: loss=10.400, acc=0.000%, last_lr=[0.001]\n",
      "\t Batch 2 / 490: loss=10.393, acc=7.515%, last_lr=[0.001]\n",
      "\t Batch 3 / 490: loss=10.381, acc=36.458%, last_lr=[0.001]\n",
      "\t Batch 4 / 490: loss=10.364, acc=39.658%, last_lr=[0.001]\n",
      "\t Batch 5 / 490: loss=10.345, acc=35.615%, last_lr=[0.001]\n",
      "\t Batch 6 / 490: loss=10.313, acc=35.789%, last_lr=[0.001]\n",
      "\t Batch 7 / 490: loss=10.277, acc=33.036%, last_lr=[0.001]\n",
      "\t Batch 8 / 490: loss=10.215, acc=34.077%, last_lr=[0.001]\n",
      "\t Batch 9 / 490: loss=10.125, acc=36.905%, last_lr=[0.001]\n",
      "\t Batch 10 / 490: loss=10.031, acc=34.598%, last_lr=[0.001]\n",
      "\t Batch 11 / 490: loss=9.877, acc=36.607%, last_lr=[0.001]\n",
      "\t Batch 12 / 490: loss=9.705, acc=37.153%, last_lr=[0.001]\n",
      "\t Batch 13 / 490: loss=9.522, acc=33.631%, last_lr=[0.001]\n",
      "\t Batch 14 / 490: loss=9.188, acc=37.227%, last_lr=[0.001]\n",
      "\t Batch 15 / 490: loss=8.923, acc=35.045%, last_lr=[0.001]\n",
      "\t Batch 16 / 490: loss=8.494, acc=35.293%, last_lr=[0.001]\n",
      "\t Batch 17 / 490: loss=8.000, acc=34.449%, last_lr=[0.001]\n",
      "\t Batch 18 / 490: loss=7.462, acc=35.218%, last_lr=[0.001]\n",
      "\t Batch 19 / 490: loss=6.947, acc=34.499%, last_lr=[0.001]\n",
      "\t Batch 20 / 490: loss=6.290, acc=34.201%, last_lr=[0.001]\n",
      "\t Batch 21 / 490: loss=5.617, acc=37.971%, last_lr=[0.001]\n",
      "\t Batch 22 / 490: loss=5.389, acc=34.697%, last_lr=[0.001]\n",
      "\t Batch 23 / 490: loss=5.230, acc=34.524%, last_lr=[0.001]\n",
      "\t Batch 24 / 490: loss=5.469, acc=35.466%, last_lr=[0.001]\n",
      "\t Batch 25 / 490: loss=5.568, acc=40.625%, last_lr=[0.001]\n",
      "\t Batch 26 / 490: loss=5.659, acc=35.045%, last_lr=[0.001]\n",
      "\t Batch 27 / 490: loss=5.687, acc=36.260%, last_lr=[0.001]\n",
      "\t Batch 28 / 490: loss=6.298, acc=34.697%, last_lr=[0.001]\n",
      "\t Batch 29 / 490: loss=5.363, acc=37.946%, last_lr=[0.001]\n",
      "\t Batch 30 / 490: loss=5.713, acc=34.003%, last_lr=[0.001]\n",
      "\t Batch 31 / 490: loss=5.505, acc=35.317%, last_lr=[0.001]\n",
      "\t Batch 32 / 490: loss=5.401, acc=33.904%, last_lr=[0.001]\n",
      "\t Batch 33 / 490: loss=5.368, acc=35.293%, last_lr=[0.001]\n",
      "\t Batch 34 / 490: loss=5.470, acc=37.153%, last_lr=[0.0001]\n",
      "\t Batch 35 / 490: loss=5.424, acc=37.450%, last_lr=[0.0001]\n",
      "\t Batch 36 / 490: loss=5.380, acc=34.673%, last_lr=[0.0001]\n",
      "\t Batch 37 / 490: loss=5.465, acc=35.144%, last_lr=[0.0001]\n",
      "\t Batch 38 / 490: loss=5.526, acc=36.806%, last_lr=[0.0001]\n",
      "\t Batch 39 / 490: loss=5.715, acc=35.565%, last_lr=[0.0001]\n",
      "\t Batch 40 / 490: loss=5.883, acc=35.243%, last_lr=[0.0001]\n",
      "\t Batch 41 / 490: loss=5.378, acc=36.756%, last_lr=[0.0001]\n",
      "\t Batch 42 / 490: loss=5.294, acc=36.508%, last_lr=[0.0001]\n",
      "\t Batch 43 / 490: loss=5.272, acc=35.417%, last_lr=[0.0001]\n",
      "\t Batch 44 / 490: loss=5.793, acc=37.525%, last_lr=[0.0001]\n",
      "\t Batch 45 / 490: loss=5.381, acc=35.218%, last_lr=[1e-05]\n",
      "\t Batch 46 / 490: loss=5.360, acc=35.541%, last_lr=[1e-05]\n",
      "\t Batch 47 / 490: loss=5.318, acc=35.689%, last_lr=[1e-05]\n",
      "\t Batch 48 / 490: loss=5.305, acc=34.772%, last_lr=[1e-05]\n",
      "\t Batch 49 / 490: loss=5.291, acc=37.351%, last_lr=[1e-05]\n",
      "\t Batch 50 / 490: loss=5.200, acc=35.243%, last_lr=[1e-05]\n",
      "\t Batch 51 / 490: loss=5.195, acc=35.516%, last_lr=[1e-05]\n",
      "\t Batch 52 / 490: loss=5.273, acc=33.532%, last_lr=[1e-05]\n",
      "\t Batch 53 / 490: loss=5.311, acc=35.293%, last_lr=[1e-05]\n",
      "\t Batch 54 / 490: loss=5.311, acc=33.978%, last_lr=[1e-05]\n",
      "\t Batch 55 / 490: loss=5.415, acc=35.417%, last_lr=[1e-05]\n",
      "\t Batch 56 / 490: loss=5.330, acc=38.740%, last_lr=[1e-05]\n",
      "\t Batch 57 / 490: loss=5.213, acc=34.028%, last_lr=[1e-05]\n",
      "\t Batch 58 / 490: loss=5.279, acc=37.078%, last_lr=[1e-05]\n",
      "\t Batch 59 / 490: loss=5.423, acc=35.367%, last_lr=[1e-05]\n",
      "\t Batch 60 / 490: loss=5.333, acc=34.449%, last_lr=[1e-05]\n",
      "\t Batch 61 / 490: loss=5.245, acc=35.813%, last_lr=[1e-05]\n",
      "\t Batch 62 / 490: loss=5.432, acc=34.127%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 63 / 490: loss=5.272, acc=36.186%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 64 / 490: loss=5.335, acc=33.532%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 65 / 490: loss=5.529, acc=32.887%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 66 / 490: loss=5.272, acc=37.004%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 67 / 490: loss=5.305, acc=35.938%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 68 / 490: loss=5.176, acc=35.069%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 69 / 490: loss=5.223, acc=36.210%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 70 / 490: loss=5.182, acc=36.111%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 71 / 490: loss=5.217, acc=35.441%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 72 / 490: loss=5.300, acc=35.069%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 73 / 490: loss=5.299, acc=33.929%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 74 / 490: loss=5.417, acc=33.209%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 75 / 490: loss=5.377, acc=36.930%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 76 / 490: loss=5.543, acc=33.978%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 77 / 490: loss=5.448, acc=32.217%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 78 / 490: loss=5.103, acc=35.838%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 79 / 490: loss=5.215, acc=34.995%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 80 / 490: loss=5.239, acc=37.004%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 81 / 490: loss=5.559, acc=33.532%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 82 / 490: loss=5.147, acc=34.995%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 83 / 490: loss=5.268, acc=36.111%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 84 / 490: loss=5.163, acc=36.582%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 85 / 490: loss=5.169, acc=33.978%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 86 / 490: loss=5.336, acc=34.152%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 87 / 490: loss=5.001, acc=36.979%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 88 / 490: loss=5.364, acc=34.325%, last_lr=[1.0000000000000002e-06]\n",
      "\t Batch 89 / 490: loss=5.149, acc=35.962%, last_lr=[1.0000000000000002e-06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m th \u001b[39m=\u001b[39m train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     batches,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49mN_EPOCHS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m batch_logits \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m batch_loss \u001b[39m=\u001b[39m loss_fn(batch_logits, batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1/shakespeare.ipynb#X21sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mwith\u001b[39;00m t\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/code/deep_learning_curriculum/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "th = train(\n",
    "    model,\n",
    "    batches,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
