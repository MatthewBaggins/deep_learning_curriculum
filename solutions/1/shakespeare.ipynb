{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on Shakespeare's works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare...\n",
      "Shakespeare text: 5392638 characters, 1991703 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "shakespeare_path = pathlib.Path(\"../data/shakespeare.txt\")\n",
    "\n",
    "if shakespeare_path.exists():\n",
    "    print(\"Loading Shakespeare...\")\n",
    "    with open(shakespeare_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    print(\"Fetching Shakespeare..\")\n",
    "    url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "    text = urlopen(url).read().decode(\"utf-8\")\n",
    "    with open(shakespeare_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "\n",
    "print(f\"Shakespeare text: {len(text)} characters, {len(tokens)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', 'The', ' ', 'Project', ' ', 'Gutenberg', ' ', 'eBook', ' ', 'of', ' ', 'The', ' ', 'Complete', ' ', 'Works', ' ', 'of', ' ', 'William']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Iterable, TypeVar\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Tokenizer:\n",
    "    d_vocab: int\n",
    "    tok2int: dict[str, int]\n",
    "    int2tok :dict[int, str]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_tokens(text: str) -> list[str]:\n",
    "        return re.split(r\"\\b\", text)\n",
    "    \n",
    "    @classmethod\n",
    "    def make(cls, text: str) -> Tokenizer:\n",
    "        tokens = cls.split_into_tokens(text)\n",
    "        token_counts = Counter(tokens)\n",
    "        d_vocab = len(token_counts) + 1 # plus BOS/EOS\n",
    "        tok2int = {tok: i for i, (tok, _) in enumerate(sorted(token_counts.items(), key=lambda x: x[1], reverse=True))}\n",
    "        int2tok = {i: tok for tok, i in tok2int.items()}\n",
    "        assert len(tok2int) == d_vocab - 1\n",
    "        return cls(d_vocab, tok2int, int2tok)\n",
    "    \n",
    "    @property\n",
    "    def eos(self) -> int:\n",
    "        return self.d_vocab - 1\n",
    "    @property\n",
    "    def token_set(self) -> set[str]:\n",
    "        return set(self.tok2int)\n",
    "    \n",
    "    def tokenize(self, text: str) -> tuple[list[str], list[int]]:\n",
    "        tokens = self.split_into_tokens(text)\n",
    "        assert set(tokens) <= self.token_set\n",
    "        token_ids = [self.tok2int[tok] for tok in tokens]\n",
    "        return tokens, token_ids\n",
    "    \n",
    "    def decode(self, inds: Iterable[int]) -> list[str]:\n",
    "        assert all(i < self.d_vocab for i in inds)\n",
    "        return [self.int2tok[i] for i in inds]\n",
    "        \n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "def split_into_pieces(xs: list[T], n_pieces: int, piece_length: int) -> list[list[T]]:\n",
    "    assert n_pieces * piece_length < len(xs)\n",
    "    # max_start_ind = len(xs) - piece_length\n",
    "    # start_inds = [random.randint(0, max_start_ind) for _ in range(n_pieces)]\n",
    "    pieces = [\n",
    "        xs[i * piece_length : (i + 1) * piece_length] \n",
    "        for i in range(n_pieces)\n",
    "    ]\n",
    "    assert len(pieces) == n_pieces\n",
    "    assert all(len(p) == piece_length for p in pieces)\n",
    "    return pieces\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer.make(text)\n",
    "tokens, token_ids = tokenizer.tokenize(text)\n",
    "\n",
    "cfg = Config(\n",
    "    d_model=128,\n",
    "    d_vocab=tokenizer.d_vocab,\n",
    "    n_layers=4,\n",
    "    n_heads=8,\n",
    "    n_ctx=256,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "pieces = split_into_pieces(token_ids, n_pieces=256, piece_length=cfg.n_ctx)\n",
    "token_tensor = t.tensor(pieces)#.unsqueeze(0)\n",
    "\n",
    "print(tokenizer.decode(pieces[0])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits: t.Tensor, tokens: t.Tensor) -> t.Tensor:\n",
    "    logits = logits[:, :-1]\n",
    "    tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens).squeeze(-1)\n",
    "    return -correct_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 256])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_BATCHES = 8\n",
    "assert token_tensor.size(0) % N_BATCHES == 0\n",
    "batches = token_tensor.reshape(N_BATCHES, -1, token_tensor.size(-1))\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: loss = 497.613\n",
      "[1]: loss = 475.608\n",
      "[2]: loss = 456.961\n",
      "[3]: loss = 438.606\n",
      "[4]: loss = 417.623\n",
      "[5]: loss = 394.853\n",
      "[6]: loss = 370.869\n",
      "[7]: loss = 356.367\n",
      "[8]: loss = 350.797\n",
      "[9]: loss = 343.557\n",
      "[10]: loss = 333.555\n",
      "[11]: loss = 320.975\n",
      "[12]: loss = 308.607\n",
      "[13]: loss = 300.223\n",
      "[14]: loss = 294.453\n",
      "[15]: loss = 289.125\n",
      "[16]: loss = 283.711\n",
      "[17]: loss = 278.533\n",
      "[18]: loss = 274.038\n",
      "[19]: loss = 269.739\n",
      "[20]: loss = 265.175\n",
      "[21]: loss = 261.107\n",
      "[22]: loss = 257.682\n",
      "[23]: loss = 253.756\n",
      "[24]: loss = 248.863\n",
      "[25]: loss = 243.134\n",
      "[26]: loss = 237.000\n",
      "[27]: loss = 231.162\n",
      "[28]: loss = 226.516\n",
      "[29]: loss = 223.017\n",
      "[30]: loss = 220.411\n",
      "[31]: loss = 218.557\n",
      "[32]: loss = 216.727\n",
      "[33]: loss = 213.388\n",
      "[34]: loss = 207.911\n",
      "[35]: loss = 201.511\n",
      "[36]: loss = 195.534\n",
      "[37]: loss = 190.740\n",
      "[38]: loss = 187.545\n",
      "[39]: loss = 185.531\n",
      "[40]: loss = 183.982\n",
      "[41]: loss = 182.198\n",
      "[42]: loss = 179.854\n",
      "[43]: loss = 177.266\n",
      "[44]: loss = 174.394\n",
      "[45]: loss = 171.092\n",
      "[46]: loss = 167.486\n",
      "[47]: loss = 163.795\n",
      "[48]: loss = 160.272\n",
      "[49]: loss = 156.875\n",
      "[50]: loss = 153.682\n",
      "[51]: loss = 150.967\n",
      "[52]: loss = 148.832\n",
      "[53]: loss = 147.037\n",
      "[54]: loss = 145.374\n",
      "[55]: loss = 143.761\n",
      "[56]: loss = 142.026\n",
      "[57]: loss = 140.126\n",
      "[58]: loss = 138.156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, batch)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Backward and update\u001b[39;00m\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthewbaggins/code/deep_learning_curriculum/solutions/1.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Append\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/code/tokensorting/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n",
      "\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n",
      "\u001b[1;32m    486\u001b[0m     )\n",
      "\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n",
      "\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n",
      "\u001b[1;32m    489\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/code/tokensorting/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n",
      "\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(cfg)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.1, patience=4)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "for epoch_i in range(N_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    for batch_i, batch in enumerate(batches):\n",
    "        # Forward\n",
    "        logits = model(batch)\n",
    "        # Loss\n",
    "        loss = loss_fn(logits, batch)\n",
    "        # Backward and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Append\n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    # Measure\n",
    "    epoch_loss = t.tensor(epoch_losses).mean().item()\n",
    "    loss_history.append(epoch_loss)\n",
    "    \n",
    "    print(f\"[{epoch_i}]: loss = {epoch_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "dt_str = datetime.now().isoformat().replace(\":\", \"-\").split(\".\")[0]\n",
    "model_filepath = f\"../models/model-1-{dt_str}.pkl\"\n",
    "with open(model_filepath, \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- add BOS token\n",
    "- retrain model with max number of splits\n",
    "- generate shakespeare or sth"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
